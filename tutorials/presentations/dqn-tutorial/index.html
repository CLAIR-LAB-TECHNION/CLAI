<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Tutorial: From Tabular Q-Learning to DQN</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/white.css" id="theme">
		<!-- Add DLR logo -->
		<link rel="stylesheet" href="css/dlr.css">
		<!-- Grid system: http://flexboxgrid.com/ -->
		<link rel="stylesheet" href="css/flexboxgrid.min.css">

		<!-- Theme used for syntax highlighted code -->
		<!-- <link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme"> -->
		<link rel="stylesheet" href="plugin/highlight/atom-one-dark.css" id="highlight-theme">
	</head>
	<body>
		<div class="side-block">
		</div>
		<div class="reveal">
			<div class="slides">
				
				<section data-background-image="images/bg_image.jpg">
					<div class="row bottom-xs">
						<div class="row middle-xs">
							<div class="col-xs-5">
								<div class="col-xs-12">
									<h3 id='main-title'>From <br> Tabular Q-Learning <br> to <br> DQN</h3>
								</div>
							</div>
							<div class="col-xs-7" style="background-color: rgba(255, 255, 255, 0.2);">
								<a target="_blank">
									<img class="shadow" src="images/q_learning/dqn_pirate_cover.png" alt="DQN cover" style="max-width:100%;">
								</a>
							</div>
						</div>
						<div class="col-xs-5 xsmall-text">
							Itay Segev 
							<br>
							<span class="italic">CLAI Tutorial 7</span><br>
							<a href="https://github.com/CLAIR-LAB-TECHNION/CLAI">https://github.com/CLAIR-LAB-TECHNION/CLAI</a>
						</div>
					</div>

				</section>

				<section>
					<h4>Outline</h4>
					<ol class="medium-text">
						<li>Tabular Q-Learning</li>
						<li>RL as a regression problem (Fitted Q Iteration)</li>
						<li>From FQI to Deep Q-Network (DQN)</li>
					</ol>
				</section>

				<section>
						 <h4>Value Functions</h4>
						 <p>How good is it to be in this state?</p>
						 <div class="row">
								 <div class="col-xs-4 fragment" data-fragment-index="1">
										 <img src="images/rl101/chess_draw.png" alt="chess draw" style="max-width: 80%">
								 </div>
								 <div class="col-xs-4 fragment" data-fragment-index="2">
										 <img src="images/rl101/chess_win.png" alt="chess win" style="max-width: 80%">
								 </div>
								 <div class="col-xs-4 fragment" data-fragment-index="3">
										 <img src="images/rl101/chess_magnus.png" alt="chess Magnus Carlsen" style="max-width: 80%">
								 </div>
						 </div>
						 <div class="row">
							 <div class="col-xs-4 fragment" data-fragment-index="1">
								 <p class="small-text">Win: 1.0 | Draw: 0.5 | Lose: 0.0</p>
							 </div>
							 <div class="col-xs-4 fragment" data-fragment-index="2">
								 <p class="small-text">Depends on the <b>state</b></p>
							 </div>
							 <div class="col-xs-4 fragment" data-fragment-index="3">
								 <p class="small-text">Depends on the <b>policy</b></p>
							 </div>

						 </div>
						 <p class="xsmall-text">Source: Freek Stulp - Master AIC</a></p>
				</section>
				<section>
						 <h5>Action-Value Function: Q-Value</h5>
						 <p>What if we have no model?</p>
						 <p class="fragment"><b>Solution</b>: $Q_\pi(s, a)$ instead of $V_\pi(s)$</p>
						 <div class="fragment" style="font-size:100%; text-align:center">
							 \[\begin{aligned}
							 Q_\pi(s, a) = \mathop{\mathbb{E}}[r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + ... | s_t=s, a_t=a].
							 \end{aligned} \]
						 </div>
						 <p class="fragment">
							 \[\begin{aligned}
							 \pi(s) = \argmax_{a \in A} Q_\pi(s, a)
							 \end{aligned} \]
						 </p>

				</section>

				<section>
				 	<h4>Tabular Q-Learning: Discrete States</h4>
					<img src="images/q_learning/discrete_state.png" alt="discrete state" style="max-width:80%;">
				</section>

				 <section>
					 <section>
					 	<h4>Tabular Q-Learning: Q-values</h4>
						<div class="r-stack">
							<img class="fragment current-visible" src="images/q_learning/rl_101.png" alt="RL 101" style="max-width:80%;">
							<img class="fragment current-visible" src="images/q_learning/q_left.png" width="70%">
							<img class="fragment current-visible" src="images/q_learning/q_straight.png" width="70%">
							<img class="fragment" src="images/q_learning/q_right.png" width="70%">
						</div>
					 </section>
				 </section>

				 <section>
					 <h4>Tabular Q-Learning: Update rule</h4>
					 <div class="row">
					 	<div class="col-xs-12 fragment">
							<p>Bellman equation for optimal value function:</p>
							<div style="font-size:70%; text-align:center">
								\[\begin{aligned}
								\textcolor{#1864ab}{Q^*(s_t, a_t)} = \mathop{\mathbb{E}}[\textcolor{#a61e4d}{r(s_t, a_t) + \gamma \max_{a'} Q^*(s_{t+1},a')}].
								\end{aligned} \]
							</div>
					 	</div>
						<div class="col-xs-12 fragment">
							<p>Q-learning update rule</p>
							<div style="font-size:70%; text-align:center">
								\[\begin{aligned}
								\textcolor{#1864ab}{Q^n(s_t, a_t)} \gets \textcolor{#1864ab}{Q^{n-1}(s_t, a_t)} + \alpha \cdot (\textcolor{#a61e4d}{r_t + \gamma \cdot \max_{a'} Q^{n-1}(s_{t+1}, a')} - \textcolor{#1864ab}{Q^{n-1}(s_t, a_t)})
								\end{aligned} \]
							</div>
						</div>
					 </div>

				 </section>

				 <section>
					<section>
						<h4>Tabular Q-Learning: Update explained</h4>
						<div class="row">
							<div class="col-xs-12 fragment">
								<p class="small-text">$\alpha=1$ (learning rate)</p>
								<div style="font-size:70%; text-align:center">
									\[\begin{aligned}
									\textcolor{#1864ab}{Q(s_t, a_t)} =  \textcolor{#a61e4d}{r_t + \gamma \cdot \max_{a'} Q(s_{t+1}, a')}
									\end{aligned} \]
								</div>
							</div>
							<div class="col-xs-12 fragment">
								<img class="" src="images/q_learning/q_equa_1.png" width="100%">

							</div>
						</div>

					</section>
					<section>
						<h4>Reminder</h4>
						<img class="" src="images/q_learning/q_equa_3.png" width="100%">
						<hr />
						<img class="" src="images/q_learning/q_equa_2.png" width="100%">

					</section>
					<section>
						<h4>Tabular Q-Learning: Terminal State</h4>
						<img src="images/q_learning/q_equa_terminal.png" alt="">
					</section> 

				 </section>

				
				 <section>
				 	<h4>Tabular Q-Learning: Limitations</h4>
					<ul>
						<li class="fragment">Discrete states</li>
						<li class="fragment">No generalization (lookup table)</li>
						<li class="fragment">Discrete actions</li>
					</ul>
				 </section>

				 <section>
				 	<h4>How to go beyond tabular Q-Learning?</h4>
					<div class="r-stack">
						<img class="fragment current-visible" src="images/fqi/tabular_limit_1.png" width="75%">
						<img class="fragment" src="images/fqi/tabular_limit_2.png" width="75%">
					</div>
				 </section> 

				 <section>
					 <h3>Q-Value Estimator</h3>
					 <div class="row">
						 <div class="col-xs-6 fragment">
							 <img src="images/q_learning/q_table.png" alt="q table" style="max-height: 80%">
						 </div>
						 <div class="col-xs-6 fragment">
							 <img src="images/fqi/q_value_fqi.png" alt="q value" style="max-height: 80%">
						 </div>
					 </div>

					 <aside class="notes">
						 maybe give equation for linear estimator?
					 </aside>
				 </section>

				 <section>
				 	<h4>Q-Learning Regression (1/2)</h4>
					<div class="row">
						<div class="col-xs-12">
							<img class="fragment" src="images/fqi/fqi_regression.png" alt="">
						</div>
						<div class="col-xs-12">
							<div class="fragment" style="font-size:100%; text-align:center">
								\[\begin{aligned}
								\textcolor{#1864ab}{Q_{\textcolor{black}{\theta}}(s_t, a_t)} = \textcolor{#a61e4d}{r_t + \gamma \cdot \max_{a' \in A}(Q_{\textcolor{black}{\theta}}(s_{t+1}, a'))}
								\end{aligned} \]
							</div>
							<div class="fragment" style="font-size:100%; text-align:center">
								\[\begin{aligned}
								\textcolor{#1864ab}{f_{\textcolor{black}{\theta}}(x)} = \textcolor{#a61e4d}{y}
								\end{aligned} \]
							</div>
							<div class="medium-text fragment">
								$\theta$: parameters of the estimator
							</div>

						</div>

					</div>
					<!-- <p class="fragment">Idea: approximate the Q-value</p> -->

				 </section>

				<section>
					<section>
						<h3>Fitted Q-Iteration</h3>
						<img class="fragment current-visible" src="images/FQI.png" alt="FQI" style="max-height:100%;">
					</section>
				</section>

				 <section>					 					
					 <section>
					 	<h4>Fitted Q-Iteration (code)</h4>
						<div class="row">
	 						<div class="col-xs-12 medium-text">
	 							<pre class="fragment"><code data-trim data-line-numbers="1-4|6|7-9|10-13|14-15|" class="python">
									initial_targets = rewards
									# Initial Q-value estimate
									qf_input = np.concatenate((states, actions))
									qf_model.fit(qf_input, initial_targets)

									for _ in range(N_ITERATIONS):
									    # Re-use Q-value model from previous iteration
									    # to create the next targets
									    next_q_values = get_max_q_values(qf_model, next_states)
									    # Non-terminal states target
									    targets[non_terminal_states] = rewards + gamma * next_q_values
									    # Special case for terminal states
									    targets[terminal_states] = rewards
									    # Update Q-value estimate
									    qf_model.fit(qf_input, targets)
	 							</code></pre>

	 						</div>
	 					</div>

					 </section>
				 </section>

				<section>
					<h3>FQI in practice - Task</h3>
				</section>

				 <section>
				 	<h3>FQI Limitations</h3>
					<ul class="medium-text">
						<li class="fragment">Offline RL</li>
						<li class="fragment">Loop over all possible actions $A$ to get next best action
							$\textcolor{#a61e4d}{a'}$:
							 \[\begin{aligned}
							 \max_{\textcolor{#a61e4d}{a' \in A}} Q_\theta(s_{t+1}, \textcolor{#a61e4d}{a'})
							 \end{aligned} \]
						 </li>
						<li class="fragment">Instability (target depends on $Q^{n-1}_\theta(s_{t+1}, a')$)</li>
					</ul>
				 </section>

			 <section>
			 	<h3>From FQI to DQN</h3>
				<ul class="medium-text">
					<li><span class="fragment">Offline RL</span> <span class="fragment">→</span> <span class="fragment">Online RL</span></li>
					<li><span class="fragment">Loop over actions</span> <span class="fragment">→</span> <span class="fragment">One forward pass to get all $Q_\theta(s, a)$</span></li>
					<li><span class="fragment">Instability</span> <span class="fragment">→</span> <span class="fragment">Target Network $Q_{\textcolor{green}{\theta'}}(s, a)$</span></li>
				</ul>
			 </section>

			 <section>
					<h3>Deep Q-Network (DQN)</h3>
					<div class="row">
						<div class="col-xs-7">
							<img src="images/dqn_nature.png" alt="DQN" style="max-height: 80%">
						</div>
						<div class="col-xs-5">
							  <!-- Note: DQN video is here but not long enough to show tunneling: https://huggingface.co/sb3/dqn-BreakoutNoFrameskip-v4/resolve/main/replay.mp4 -->
								<video style="width: 80%" src="https://huggingface.co/sb3/a2c-BreakoutNoFrameskip-v4/resolve/main/replay.mp4" controls></video>
						</div>
					</div>
				</section>

				<section>
					<h3>Q-network</h3>
					<div class="row">
						<div class="col-xs-6 fragment">
							<img src="images/fqi/q_value_fqi.png" alt="q value" style="max-height: 70%">
						</div>
						<div class="col-xs-6 fragment">
							<img src="images/dqn/q_network.png" alt="q network" style="max-height: 100%">
						</div>
					</div>
				</section>

				<section>
					<h3>Online Q iteration</h3>
				   <div class="row">
					   <div class="col-xs-12">
						   <img src="images/online_q_learing.png" alt="online_q_learing" style="max-height: 70%">
					   </div>
				   </div>
			   </section>
			   <section>
				<h3>Online Q iteration</h3>
			   <div class="row">
				   <div class="col-xs-12">
					   <img src="images/online_qlearning_problems.png" alt="online_q_learing_problems" style="max-height: 70%">
				   </div>
			   </div>
		   </section>



				<section>
					<h3>Replay Buffer</h3>
					<div class="row">
						<div class="col-xs-12">
							<img src="images/dqn/replay_buffer.png" alt="Replay" style="max-height: 90%">
						</div>
					</div>
				</section>

				<section>
					<h3>Replay Buffer Sampling</h3>
					<div class="row">
						<div class="col-xs-12">
							<img src="images/dqn/replay_buffer_sampling.png" alt="Replay sampling" style="max-height: 60%">
						</div>
					</div>
				</section>


 				<section>
 					<h3>Collecting Experience</h3>
					<div class="row">
						<div class="col-xs-12" style="font-size: 90%">
							<pre><code data-noescape data-trim data-line-numbers="1-2|4-6|8-9|11-12|" class="python">
								# Retrieve q values for the current observation
								q_values = q_model(current_obs)

								# Follow greedy-policy:
								# take the action with the highest q_value
								action = np.argmax(q_values)

								# Do one step in the env
								next_obs, reward, terminated, _, _ = env.step(action)

								# Store transition in the replay buffer
								replay_buffer.store(obs, action, reward, terminated, next_obs)
							</code></pre>

						</div>
					</div>

				</section>
 				
				<section>
 					<h3>Target Q-Network</h3>
 					<div class="row">
 						<div class="col-xs-12">
 							<img src="images/dqn/target_q_network.png" alt="DQN" style="max-height: 100%">
 						</div>
 					</div>
 				</section>

				<section>
 					<h3>DQN Overview</h3>
 					<div class="row">
 						<div class="col-xs-12">
 							<img src="images/dqn/dqn.png" alt="DQN" style="max-height: 80%">
 						</div>
 					</div>
 				</section>

				<section>
					<h3>DQN Overview</h3>
					<div class="row">
						<div class="col-xs-12">
							<img src="images/dqn/fixed-q-target-pseudocode.jpg" alt="DQN" style="max-height: 80%">
						</div>
					</div>
				</section>

				
				<section>
				 	<h2>Backup Slides</h2>
				</section>

				<section>
					<h4>Improving DQN</h4>
				   <ul>
					   <li class="fragment">Double DQN</li>
					   <li class="fragment">Multi-step returns</li>
					   <li class="fragment">DQN with Continuous Actions - DDPG, Twin Delayed DDPG </li>
				   </ul>
				</section>

				<section>
					<h4>Practical Tips</h4>
				   <ul>
					   <li class="fragment">DQN takes some care to stabilize</li>
					   <li class="fragment">Large replay buffers help improve stability</li>
					   <li class="fragment">Start with high exploration (epsilon) and gradually reduce</li>
					   <li class="fragment">Bellman error gradients can be big; clip gradients or use Huber loss</li>
					   <li class="fragment">Double DQN helps a lot in practice</li>
					   <li class="fragment">Adam optimizer</li>
					</ul>
				</section>

				<section>
					<h3>Exploration / Exploitation</h3>
				   <div class="row">
					   <div class="col-xs-12">
						   <img src="images/dqn/epsilon_greedy.png" alt="DQN" style="max-height: 70%">
					   </div>
				   </div>
			   </section>

			   <section>
				<h3>Epsilon-greedy Exploration</h3>
				<div class="row">
					<div class="col-xs-12" style="font-size: 90%">
						<pre><code data-noescape data-trim data-line-numbers="1-2|4-6|7-9|" class="python">
							# Flip a biased coin
							take_random_action = np.random.rand() < exploration_rate

							if take_random_action:
								# Random action
								action = action_space.sample()
							else:
								# Greedy action
								action = np.argmax(q_values)
						</code></pre>

					</div>
				</div>

			</section>

			<section>
				<h3>Exploration Schedule</h3>
				<div class="row">
					<div class="col-xs-12">
						<img src="images/dqn/linear_schedule.png" alt="Linear Schedule" style="max-height: 70%">
					</div>
				</div>
			</section>

				<section>
					<section>
						<h3>Annotated DQN Algorithm</h3>
					   <div class="r-stack">
						   <img class="fragment current-visible" src="images/dqn/dqn_equa.png" alt="DQN" style="max-height:100%;">
						   <img class="fragment" src="images/dqn/annotated_dqn.png" style="max-height:100%;">
					   </div>
					</section>
				</section>

				<section>
					<section>
						<h3>Regression 101</h3>
						<img src="images/regression/regression.png" alt="">
					</section>
					<section data-transition="fade">
						<h4>Linear model</h4>
						<img src="images/regression/linear_model.png" alt="">
					</section>
					<section data-transition="fade">
						<h4>Linear model(s)?</h4>
						<div style="font-size:60%; text-align:center">
							\[\begin{aligned}
							 \textcolor{#a61e4d}{y} = \textcolor{#1864ab}{f_\theta(x)} \quad ; \quad
							 \theta = \{\text{slope}, \text{bias}\}
							\end{aligned} \]
						</div>
						<img width="80%" src="images/regression/linear_regression_models.png" alt="">
					</section>
					<section data-transition="fade">
						<h4>Non Linear Model</h4>
						<img src="images/regression/non_linear_model.png" alt="">
					</section>
				</section>

				<section>
					<h3>Scikit-Learn API</h3>
					<div class="row">
						<div class="col-xs-12" style="font-size: 100%">
							<pre><code data-noescape data-trim data-line-numbers="1-2|3-5|6-8|9-10|" class="python">
								import numpy as np
								from sklearn.linear_model import LinearRegression
								# Generate some data (noisy linear function)
								x = np.linspace(0, 5, num=50).reshape(50, 1)
								y = 2 * x + 10 + 0.1 * np.random.rand()
								# Fit a linear model using least squares
								model = LinearRegression().fit(x, y)
								y_predict = model.predict(x)
								# Retrieve the optimized parameters
								slope, bias = model.coef_, model.intercept_
							</code></pre>

						</div>
					</div>

				</section>

				<section>
					<h3>Stable-Baselines3 (SB3)</h3>
				   <div class="row">
					   <div class="col-xs-12" style="font-size: 100%">
						   <pre><code data-noescape data-trim data-line-numbers="|" class="python">
							   from stable_baselines3 import DQN
							   # SAC, TD3, TQC are all successors of DQN
							   from stable_baselines3 import SAC, TD3
							   from sb3_contrib import TQC

							   # Instantiate the algorithm on the Lunar Lander env
							   model = DQN("MlpPolicy", "LunarLander-v2", verbose=1)
							   # Train for 100 000 steps
							   model.learn(100_000, progress_bar=True)
						   </code></pre>

					   </div>
					   <div class="medium-text col-xs-12">
						   <p>
							   <a href="https://github.com/DLR-RM/stable-baselines3">https://github.com/DLR-RM/stable-baselines3</a>
						   </p>
					   </div>
				   </div>
			    </section>

			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				// Display the page number of the current slide
				slideNumber: true,

				// Add the current slide number to the URL hash so that reloading the
				// page/copying the URL will return you to the same slide
				hash: true,

				// Push each slide change to the browser history. Implies `hash: true`
				// history: false,

				// math: {
				// 	mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
				// 	config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
				// 	// pass other options into `MathJax.Hub.Config()`
				// 	// TeX: { Macros: macros }
				// },

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX]
			});
		</script>
	</body>
</html>
