{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c79b2ed6"
      },
      "source": [
        "<div style=\"text-align: left\">\n",
        "    <img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/logo.png?raw=true' width=800/>  \n",
        "</div>\n",
        "\n",
        "Author: Itay Segev\n",
        "\n",
        "E-mail: [itaysegev@campus.technion.ac.il](mailto:itaysegev@campus.technion.ac.il)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIdLEDqyzdLk"
      },
      "source": [
        "# Value Based Methods\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/tut04_creation_of_ai.png?raw=true' width=900/>"
      ],
      "metadata": {
        "id": "hp1H54SQLRUU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8c04ad1-e844-4836-9de1-206dc454d870",
        "tags": []
      },
      "source": [
        "\n",
        "<a id=\"section:intro\"></a>\n",
        "\n",
        "# <img src=\"https://img.icons8.com/?size=50&id=55412&format=png&color=000000\" style=\"height:50px;display:inline\"> Introduction\n",
        "---\n",
        "\n",
        "In reinforcement learning, **value-based methods** are crucial for estimating the value of states and actions, helping agents make optimal decisions. This tutorial covers key concepts in value-based methods, focusing on **Tabular Q-Learning**. Value functions, including the state-value function $V(s)$ and action-value function $Q(s, a)$, represent the expected return of states and actions, guiding the agent towards the optimal policy $\\pi^*$. Tabular Q-Learning, a foundational algorithm, uses a Q-table to store Q-values for state-action pairs, updating them through temporal difference learning to find the optimal action-value function $Q^*(s, a)$. Key topics include Bellman equations, temporal difference learning, the Q-Learning algorithm, and balancing exploration and exploitation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oid7uNNGKvv"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/?size=50&id=43171&format=png&color=000000\" style=\"height:30px;display:inline\"> Setup\n",
        "\n",
        "\n",
        "You will need to make a copy of this notebook in your Google Drive before you can edit the notebook. You can do so with **File &rarr; Save a copy in Drive**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "DSNObDRJgu8b"
      },
      "outputs": [],
      "source": [
        "#@title mount your Google Drive\n",
        "import os\n",
        "connect_drive = False #@param {type: \"boolean\"}\n",
        "if connect_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "  # set up mount symlink\n",
        "  DRIVE_PATH = '/content/gdrive/My\\ Drive/cs236203_s24'\n",
        "  DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "  if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "    %mkdir $DRIVE_PATH\n",
        "\n",
        "## the space in `My Drive` causes some issues,\n",
        "## make a symlink to avoid this\n",
        "SYM_PATH = '/content/cs236203_s24'\n",
        "if not os.path.exists(SYM_PATH) and connect_drive:\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ft-NJZ29L_p9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title apt install requirements\n",
        "\n",
        "#@markdown Run each section with Shift+Enter\n",
        "\n",
        "#@markdown Double-click on section headers to show code.\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!apt install ffmpeg xvfb\n",
        "!pip3 install pyvirtualdisplay\n",
        "!pip install gymnasium\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make sure the new installed libraries are used, **sometimes it's required to restart the notebook runtime**. The next cell will force the **runtime to crash, so you'll need to connect again and run the code starting from here**. Thanks to this trick, **we will be able to run our virtual screen.**"
      ],
      "metadata": {
        "id": "K6XC13pTfFiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "3kuZbWAkfHdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "B8_R2qRiG2xA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da85ad1c-cf15-4b7d-bb99-ffb2a29c449e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f398b409f90>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#@title set up virtual display\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-7f-Swax_9x"
      },
      "source": [
        "## Import the packages\n",
        "\n",
        "In addition to the installed libraries, we also use:\n",
        "\n",
        "- `random`: To generate random numbers (that will be useful for epsilon-greedy policy).\n",
        "- `imageio`: To generate a replay video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2bcb5e2f-b9e0-4354-a24a-2f893b1313e6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import imageio\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=50&id=8GlFeky9HXyQ&format=png&color=000000\" style=\"height:30px;display:inline\"> The FrozenLake-v1 Environment\n",
        "\n",
        "\n",
        "\n",
        "The [Frozen Lake environment](https://gymnasium.farama.org/environments/toy_text/frozen_lake) is a classic navigation task in reinforcement learning. It is designed to test the basic capabilities of an RL algorithm in a grid world setting. Here‚Äôs a detailed description of the environment:"
      ],
      "metadata": {
        "id": "XgydhMV1q8DV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to train our Q-Learning agent **to navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H)**.\n",
        "\n",
        "We can have two sizes of environment:\n",
        "\n",
        "- `map_name=\"4x4\"`: a 4x4 grid version\n",
        "- `map_name=\"8x8\"`: a 8x8 grid version\n",
        "\n",
        "\n",
        "The environment has two modes:\n",
        "\n",
        "- `is_slippery=False`: The agent always moves **in the intended direction** due to the non-slippery nature of the frozen lake (deterministic).\n",
        "- `is_slippery=True`: The agent **may not always move in the intended direction** due to the slippery nature of the frozen lake (stochastic).\n"
      ],
      "metadata": {
        "id": "mUkOD7xsq8Dd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://gymnasium.farama.org/_images/frozen_lake.gif' width=600/>"
      ],
      "metadata": {
        "id": "VnSBVFDMq8Dd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaW_LHfS0PY2"
      },
      "source": [
        "For now let's keep it simple with the 4x4 map and non-slippery.\n",
        "We add a parameter called `render_mode` that specifies how the environment should be visualised. In our case because we **want to record a video of the environment at the end, we need to set render_mode to rgb_array**.\n",
        "\n",
        "As [explained in the documentation](https://gymnasium.farama.org/api/env/#gymnasium.Env.render) ‚Äúrgb_array‚Äù: Return a single frame representing the current state of the environment. A frame is a np.ndarray with shape (x, y, 3) representing RGB values for an x-by-y pixel image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Wi_HrvKaq8Dd"
      },
      "outputs": [],
      "source": [
        "env_id = \"FrozenLake-v1\"\n",
        "# Create the env\n",
        "env = gym.make(env_id, map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=46589&format=png&color=000000\" style=\"height:30px;display:inline\"> Task 1: Getting Familiar with the FrozenLake Environment\n",
        "\n"
      ],
      "metadata": {
        "id": "W-xMxm468-sh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/task_sign.png?raw=true' width=800/>"
      ],
      "metadata": {
        "id": "FA86BbfNJ8Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, you will explore the FrozenLake-v1 environment. Your objective is to understand the basic operations of this environment, such as resetting the environment, taking actions, and observing the outcomes.\n",
        "\n",
        "Questions:\n",
        "- What are the dimensions of the state space?\n",
        "- What are the possible actions in the action space?\n",
        "- What triggers the termination of an episode in FrozenLake-v1?"
      ],
      "metadata": {
        "id": "B41wcOSe9PSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/solution_sign.png?raw=true' width=800/>"
      ],
      "metadata": {
        "id": "FP8HIhmRHPtF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <img src=\"https://img.icons8.com/?size=50&id=42816&format=png&color=000000\" style=\"height:30px;display:inline\">  Solution\n",
        "\n"
      ],
      "metadata": {
        "id": "y3sRnN289aRW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zetjSAONq8Dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "289a1ac8-db00-4365-910a-300e91dc7290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_____OBSERVATION SPACE_____ \n",
            "\n",
            "Observation Space Discrete(16)\n",
            "Sample observation 2\n"
          ]
        }
      ],
      "source": [
        "# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space\", env.observation_space)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MXc15qFE0M9"
      },
      "source": [
        "We see with `Observation Space Shape Discrete(16)` that the observation is an integer representing the **agent‚Äôs current position as current_row * ncols + current_col (where both the row and col start at 0)**.\n",
        "\n",
        "For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15. The number of possible observations is dependent on the size of the map. **For example, the 4x4 map has 16 possible observations.**\n",
        "\n",
        "\n",
        "For instance, this is what state = 0 looks like:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ],
      "metadata": {
        "id": "sCpHffOXEP2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1148104e-4178-4719-e58a-dd29a538e4cc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " _____ACTION SPACE_____ \n",
            "\n",
            "Action Space Shape Discrete(4)\n",
            "Action Space Sample 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The action space is **discrete**, consisting of four possible actions:\n",
        "\n",
        "1. Move Left: Move the agent to the left.\n",
        "2. Move Down: Move the agent downwards.\n",
        "3. Move Right: Move the agent to the right.\n",
        "4. Move Up: Move the agent upwards."
      ],
      "metadata": {
        "id": "3nzCU0NJq8Dd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The agent receives a reward üí∞ of +1 for reaching the goal tile and 0 otherwise. The goal is to maximize the total reward over an episode."
      ],
      "metadata": {
        "id": "vka-_FOkq8De"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An episode ends if any of the following conditions are met:\n",
        "\n",
        "- The agent reaches the goal tile.\n",
        "- The agent falls into a hole.\n",
        "- The episode reaches a maximum number of steps."
      ],
      "metadata": {
        "id": "bpCN_y5-q8De"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KASNViqL4tZn"
      },
      "source": [
        "You can create your own custom grid like this:\n",
        "\n",
        "```python\n",
        "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
        "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n",
        "```\n",
        "\n",
        "but we'll use the default environment for now."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=50&id=53819&format=png&color=000000\" style=\"height:30px;display:inline\"> Value-based methods\n",
        "\n"
      ],
      "metadata": {
        "id": "UoXl-zQ5bxxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal is to find an optimal policy $\\pi^*$ , aka., a policy that leads to the best expected cumulative reward.\n",
        "\n",
        "And to find this optimal policy (hence solving the RL problem), there are two main types of RL methods:\n",
        "\n",
        "* Policy-based methods: Train the policy directly to learn which action to take given a state.\n",
        "\n",
        "* Value-based methods: Train a value function to learn which state is more valuable and use this value function to take the action that leads to it.\n",
        "\n"
      ],
      "metadata": {
        "id": "jBizqOjAcbpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of **value-based methods**, you don‚Äôt train the policy: your policy is just a simple pre-specified function (for instance, the Greedy Policy) that uses the values given by the value-function to select its actions.\n",
        "\n",
        "In **policy-based** training, the optimal policy (denoted $\\pi^*$) is found by training the policy directly.\n",
        "In value-based training, finding an optimal value function (denoted $Q^*$ or $V^*$, we‚Äôll study the difference below) leads to having an optimal policy."
      ],
      "metadata": {
        "id": "d0ECENBicnFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches.jpg' width=800/>"
      ],
      "metadata": {
        "id": "WS0lxfHkb2tk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=110869&format=png&color=000000\" style=\"height:30px;display:inline\"> Value Functions\n",
        "\n",
        "\n",
        "In reinforcement learning, value functions are essential mathematical constructs that help us design and conceptualize algorithms. They provide a way to evaluate the expected return (sum of rewards) starting from a given state or state-action pair and following a particular policy. Value functions are crucial for understanding and optimizing the reinforcement learning objective.\n"
      ],
      "metadata": {
        "id": "r4P79ZolWypl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of Value Functions\n",
        "\n",
        "There are four primary value functions to consider:\n",
        "\n",
        "1. **On-Policy Value Function $V^{\\pi}(s)$**: This function gives the expected return if you start in state $s$ and always act according to policy $\\pi$. Mathematically, it is defined as:\n",
        "\n",
        "   $\n",
        "   V^{\\pi}(s) = \\mathbb{E}_{\\tau \\sim \\pi}[R(\\tau) \\mid s_0 = s]\n",
        "   $\n",
        "\n",
        "   where $R(\\tau)$ is the return of trajectory $\\tau$.\n",
        "\n",
        "2. **On-Policy Action-Value Function $Q^{\\pi}(s,a)$**: This function provides the expected return if you start in state $s$, take an arbitrary action $a$ (which may not have come from the policy), and then forever after act according to policy $\\pi$:\n",
        "\n",
        "   $\n",
        "   Q^{\\pi}(s,a) = \\mathbb{E}_{\\tau \\sim \\pi}[R(\\tau) \\mid s_0 = s, a_0 = a]\n",
        "   $\n",
        "\n",
        "3. **Optimal Value Function $V^*(s)$**: This function gives the expected return if you start in state $s$ and always act according to the optimal policy in the environment:\n",
        "\n",
        "   $\n",
        "   V^*(s) = \\max_{\\pi} \\mathbb{E}_{\\tau \\sim \\pi}[R(\\tau) \\mid s_0 = s]\n",
        "   $\n",
        "\n",
        "4. **Optimal Action-Value Function $Q^*(s,a)$**: This function provides the expected return if you start in state $s$, take an arbitrary action $a$, and then forever after act according to the optimal policy in the environment:\n",
        "\n",
        "   $\n",
        "   Q^*(s,a) = \\max_{\\pi} \\mathbb{E}_{\\tau \\sim \\pi}[R(\\tau) \\mid s_0 = s, a_0 = a]\n",
        "   $\n"
      ],
      "metadata": {
        "id": "sT-X2lz0W54X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recursive Nature of Value Functions\n",
        "\n",
        "Value functions can be written recursively, leveraging the concept of causality‚Äîdecisions at a current time step do not affect past rewards. This recursive nature allows us to break down the expected return into a series of nested expectations.\n",
        "\n",
        "For example, the on-policy action-value function $Q^{\\pi}(s,a)$ can be expressed as:\n",
        "\n",
        "$\n",
        "Q^{\\pi}(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s'}[V^{\\pi}(s') \\mid s, a]\n",
        "$\n",
        "\n",
        "where $r(s,a)$ is the immediate reward and $\\gamma$ is the discount factor.\n",
        "\n",
        "Similarly, the on-policy value function $V^{\\pi}(s)$ can be written in terms of the action-value function:\n",
        "\n",
        "$\n",
        "V^{\\pi}(s) = \\mathbb{E}_{a \\sim \\pi}[Q^{\\pi}(s,a)]\n",
        "$"
      ],
      "metadata": {
        "id": "suIibwsuYGrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connections Between Value Functions\n",
        "\n",
        "There are two key connections between the value function and the action-value function:\n",
        "\n",
        "1. **Expected Value**:\n",
        "   $\n",
        "   V^{\\pi}(s) = \\mathbb{E}_{a \\sim \\pi}[Q^{\\pi}(s,a)]\n",
        "   $\n",
        "   This relation shows that the value of a state under policy $\\pi$ is the expected value of the action-value function over actions taken according to $\\pi$.\n",
        "\n",
        "2. **Optimal Value**:\n",
        "   $\n",
        "   V^*(s) = \\max_a Q^*(s,a)\n",
        "   $\n",
        "   This relation indicates that the optimal value of a state is the maximum action-value function over all possible actions."
      ],
      "metadata": {
        "id": "86i0TDAjYiTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importance of Value Functions\n",
        "\n",
        "Value functions play a critical role in reinforcement learning for several reasons:\n",
        "\n",
        "1. **Policy Evaluation**: They allow us to evaluate how good a policy is by estimating the expected return starting from a particular state or state-action pair.\n",
        "2. **Policy Improvement**: By comparing value functions under different policies, we can systematically improve the policy. This is the basis of methods like policy iteration and Q-learning.\n",
        "3. **Reducing Variance**: Using value functions, we can derive estimators with lower variance, which are more stable and reliable for learning.\n"
      ],
      "metadata": {
        "id": "5U_m_15UYzs9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "tRKj1uPfzdLs"
      },
      "source": [
        "### Value of a trajectory / of a policy\n",
        "\n",
        "An oracle decides on how to choose actions at each time step:\n",
        "$$\\mathbb{P}(A_t)=\\pi_t.$$\n",
        "\n",
        "The collection of $\\pi_t$ is the oracle's **policy**.\n",
        "\n",
        "\n",
        "One policy implies one specific distribution over trajectories over the frozen lake. More generally, the policy and $S_0$ condition the sequence $S_0, A_0, R_0, S_1, A_1, R_1, \\ldots$\n",
        "\n",
        "In FrozenLake some trajectories are better than others. **We need a criterion to compare trajectories.** Intuitively, this criterion should reflect the idea that a good policy accumulates as much reward as possible along a trajectory.\n",
        "\n",
        "Let's compare the policy that always moves to the right and the policy that always moves left by summing the rewards obtained along trajectories and then averaging these rewards across trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "hidden": true,
        "id": "H6wJU3HSzdLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cd0b7f4-232d-43c2-e256-a78ee5bac07f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "est. value of 'right' policy: 0.0 variance: 0.0\n",
            "est. value of 'left'  policy: 0.0 variance: 0.0\n"
          ]
        }
      ],
      "source": [
        "import gymnasium.envs.toy_text.frozen_lake as fl\n",
        "\n",
        "nb_episodes = 50000\n",
        "horizon = 200\n",
        "\n",
        "Vright = np.zeros(nb_episodes)\n",
        "for i in range(nb_episodes):\n",
        "    env.reset()\n",
        "    for t in range(horizon):\n",
        "        next_state, r, terminated, truncated,_ = env.step(fl.RIGHT)\n",
        "        Vright[i] += r\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "Vleft  = np.zeros(nb_episodes)\n",
        "for i in range(nb_episodes):\n",
        "    env.reset()\n",
        "    for t in range(horizon):\n",
        "        next_state, r, terminated, truncated,_ = env.step(fl.LEFT)\n",
        "        Vleft[i] += r\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "print(\"est. value of 'right' policy:\", np.mean(Vright), \"variance:\", np.std(Vright))\n",
        "print(\"est. value of 'left'  policy:\", np.mean(Vleft),  \"variance:\", np.std(Vleft))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Ax3kkeVUzdLt"
      },
      "source": [
        "In the general case, this sum of rewards on an infinite horizon might be unbounded. So let us introduce the **$\\gamma$-discounted sum of rewards** (from a starting state $s$, under policy $\\pi$) random variable:\n",
        "$$G^\\pi(s) = \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s,\\\\ A_t \\sim \\pi_t,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
        "\n",
        "$G^\\pi(s)$ represents what we can gain in the long-term by applying the actions from $\\pi$.\n",
        "\n",
        "Then, given a starting state $s$, we can define the value of $s$ under policy $\\pi$:\n",
        "$$V^\\pi(s) = \\mathbb{E} \\left[ G^\\pi(s) \\right]$$\n",
        "\n",
        "This defines the value function $V^\\pi$ of policy $\\pi$:\n",
        "<div class=\"alert alert-success\"><b>Value function $V^\\pi$ of a policy $\\pi$ under a $\\gamma$-discounted criterion</b><br>\n",
        "$$V^\\pi : \\left\\{\\begin{array}{ccl}\n",
        "S & \\rightarrow & \\mathbb{R}\\\\\n",
        "s & \\mapsto & V^\\pi(s)=\\mathbb{E}\\left( \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\bigg| S_0 = s, \\pi \\right)\\end{array}\\right. $$\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this definition is quite arbitrary: instead of the expected (discounted) sum of rewards, we could have taken the average reward over all time steps, or some other (more or less exotic) comparison criterion between policies.\n",
        "\n",
        "Most of the RL literature uses this discounted criterion (in some cases with $\\gamma=1$), some uses the average reward criterion, and few works venture into more exotic criteria. Today, we will limit ourselves to the discounted criterion."
      ],
      "metadata": {
        "id": "_Uo9VeqRvFZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Optimal Q-Function and the Optimal Action\n",
        "\n",
        "There is a significant connection between the optimal action-value function $Q^*(s,a)$ and the action selected by the optimal policy. By definition, $Q^*(s,a)$ gives the expected return for starting in state $s$, taking an arbitrary action $a$, and then following the optimal policy thereafter.\n",
        "\n",
        "The optimal policy in state $s$ will select the action that maximizes the expected return from $s$. Therefore, if we have $Q^*$, we can directly obtain the optimal action $a^*(s)$ as:\n",
        "\n",
        "$$a^*(s) = \\arg \\max_a Q^*(s,a)$$\n",
        "\n",
        "Note that there may be multiple actions which maximize $Q^*(s,a)$, in which case, all of them are optimal, and the optimal policy may randomly select any of them. However, there is always an optimal policy that deterministically selects an action."
      ],
      "metadata": {
        "id": "5Q17R2qOgD-E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "pbIQ15HYzdLu"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=77996&format=png&color=000000\" style=\"height:30px;display:inline\"> Bellman Equations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In reinforcement learning, all four of the value functions obey special self-consistency equations called Bellman equations. The basic idea behind the Bellman equations is:\n",
        "\n",
        "The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.\n",
        "\n",
        "\n",
        "\n",
        "The Bellman equations for the on-policy value functions are:\n",
        "\n",
        "$$\n",
        "V^{\\pi}(s) = \\mathbb{E}_{a \\sim \\pi, \\, s' \\sim P}[r(s,a) + \\gamma V^{\\pi}(s')]\n",
        "$$\n",
        "\n",
        "$$\n",
        "Q^{\\pi}(s,a) = \\mathbb{E}_{s' \\sim P}[r(s,a) + \\gamma \\mathbb{E}_{a' \\sim \\pi}[Q^{\\pi}(s',a')]]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "The Bellman equations for the optimal value functions are:\n",
        "\n",
        "$$\n",
        "V^*(s) = \\max_a \\mathbb{E}_{s' \\sim P}[r(s,a) + \\gamma V^*(s')]\n",
        "$$\n",
        "\n",
        "$$\n",
        "Q^*(s,a) = \\mathbb{E}_{s' \\sim P}[r(s,a) + \\gamma \\max_{a'} Q^*(s',a')]\n",
        "$$\n",
        "\n",
        "The crucial difference between the Bellman equations for the on-policy value functions and the optimal value functions is the absence or presence of the $\\max$ over actions. Its inclusion reflects the fact that whenever the agent gets to choose its action, in order to act optimally, it has to pick whichever action leads to the highest value.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qZRAfeWfyw-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, which is a long process, we calculate the value as the sum of the immediate reward plus the discounted value of the state that follows."
      ],
      "metadata": {
        "id": "EyF_qHA_zAcx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "eypSYcGfzdLu"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/?size=50&id=46812&format=png&color=000000\" style=\"height:30px;display:inline\"> Intuitions\n",
        "\n",
        "The following section is designed to enhance your understanding of value functions and their applications. While it includes examples you may have encountered in an intro to AI course, we highly recommend reviewing this material. This foundation will ensure you are comfortable navigating the different ideas and terms used in value-based reinforcement learning methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "_qAj1xNbzdLu"
      },
      "source": [
        "Consider the maze below, where an agent can move North, South, East or West. The resulting transition is deterministic and a reward of $+1$ is gained when exiting the maze (which terminates the game). Otherwise all rewards are zero. Bumping into a wall terminates the game with a reward of zero.\n",
        "\n",
        "<img src=\"https://github.com/RL-VS/rlvs2021/blob/main/docs/class-material/rl_fundamentals/img/grid_raw.png?raw=1\" width=\"200px\"></img>\n",
        "\n",
        "Let's consider the policy $\\pi$ that always moves East.\n",
        "\n",
        "<img src=\"https://github.com/RL-VS/rlvs2021/blob/main/docs/class-material/rl_fundamentals/img/grid_policy.png?raw=1\" width=\"200px\"></img>\n",
        "\n",
        "\n",
        "**Without writing any equation, what is the value of the top-right cell under this policy?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "BN_6Im5BzdLu"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "$V^\\pi((3,3)) = 1$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "a6MUgd_VzdLu"
      },
      "source": [
        "**Now let's take $\\gamma=0.9$.**\n",
        "\n",
        "Without writing any equation, what is the value of the top-middle cell under this policy? What is the value of the bottom-right cell?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "xu0N1NE7zdLv"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "The value of $(2,3)$ is the expected discounted sum of what one gets from applying $\\pi$ from $(2,3)$. Since the $\\pi$ is deterministic and the transitions are deterministic too, $\\pi(2,3)$ always take us to state $(3,3)$. So $V^\\pi((2,3)) = 0 + \\gamma \\times V^\\pi((3,3)) = 0.9$.\n",
        "    \n",
        "The value of $(3,1)$ is the expected infinite sum of discounted rewards from $(3,1)$. Since the agent keeps bumping into the wall when applying $\\pi$, it never exits the maze and this is an infinite sum of zero terms. Hence $V^\\pi((2,3)) = 0$.\n",
        "</div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "dOYApNT8zdLv"
      },
      "source": [
        "Let's draw the value function.\n",
        "\n",
        "<img src=\"https://github.com/RL-VS/rlvs2021/blob/main/docs/class-material/rl_fundamentals/img/grid_vpi.png?raw=1\" width=\"200px\"></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "jVjbyAOVzdLv"
      },
      "source": [
        "Suppose you are currently in cell $(1,2)$ and would like to choose what action to take. Suppose also that you know the value function above. You need to put a scalar value on all four actions. To evaluate each action, let's estimate what we can get by applying the action and then using $\\gamma \\times V^\\pi(s)$ to estimate what can obtain in the long run after this first action. Define $Q^\\pi((x,y),a)$ as the utility we estimate for each action $a$ in $(x,y)$.\n",
        "\n",
        "**Question**  \n",
        "What is $Q^\\pi((1,2),a)$ for action $a$ in $\\{N,S,E,W\\}$? What seems to be the most interesting first action to take, if we follow $\\pi$ after?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "wV6IAznRzdLv"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "$Q^\\pi((1,2),N) = 0 + \\gamma \\cdot \\gamma^2 = 0.729$  \n",
        "$Q^\\pi((1,2),S) = 0 + \\gamma \\cdot 0 = 0$  \n",
        "$Q^\\pi((1,2),E) = 0 + \\gamma \\cdot 0 = 0$  \n",
        "$Q^\\pi((1,2),W) = 0$  \n",
        "The best action seems to be $N$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "iKpJOfz4zdLv"
      },
      "source": [
        "An optimal policy is quite easy to guess. Let's draw the optimal value function (the value function of any optimal policy).\n",
        "\n",
        "<img src=\"https://github.com/RL-VS/rlvs2021/blob/main/docs/class-material/rl_fundamentals/img/grid_vopt.png?raw=1\" width=\"200px\"></img>\n",
        "\n",
        "Define $Q^*((x,y),a)$ as the utility we estimate for each action $a$ in $(x,y)$ if it is followed by an optimal policy.\n",
        "<div class=\"alert alert-warning\">\n",
        "    \n",
        "**Question**   \n",
        "What is $Q^*((1,2),a)$ for action $a$ in $\\{N,S,E,W\\}$? What seems to be the most interesting first action to take, if we act optimally after? Rank the actions by utility.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "m_UON4kezdLv"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "$Q^*$ is what we gain immediately, plus $\\gamma$ times what we expect to receive from applying an optimal policy in the state we reach by applying $a$.  \n",
        "$Q^*((1,2),N) = 0 + \\gamma\\times\\gamma^2=\\gamma^3$  \n",
        "$Q^*((1,2),S) = 0 + \\gamma\\times\\gamma^4=\\gamma^5$  \n",
        "$Q^*((1,2),E) = 0 + \\gamma\\times\\gamma^4=\\gamma^5$  \n",
        "$Q^*((1,2),W) = 0 + \\gamma\\times\\gamma^3=\\gamma^4$  \n",
        "The best action seems to be $N$, followed by $W$, after that $S$ and $E$ are tied.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "34FskohKzdLv"
      },
      "source": [
        "\n",
        "**Question**  \n",
        "Now suppose $(1,2)$ is a special slippery cell. Going North has a $0.7$ probability of actually reaching $(1,3)$, but also a $0.2$ probability of staying in $(1,2)$ and a $0.1$ probability of ending in $(2,2)$. Note that this changes the problem and the optimal expected return function $V^*$.\n",
        "\n",
        "\n",
        "Given this new problem, can you write $Q^*((1,2),N)$ as a function of $V^*(1,3)$, $V^*(1,2)$ and $V^*(2,2)$?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "dn910aTDzdLv"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "When we take action $N$ in $(1,2)$, there are 3 possible outcomes:\n",
        "- with probability $0.7$, reach $(1,3)$ and get reward $0$,\n",
        "- with probability $0.2$, reach $(1,2)$ and get reward $0$,\n",
        "- with probability $0.1$, reach $(2,2)$ and get reward $0$.\n",
        "\n",
        "So what we can expect to get from applying $N$ in $(1,2)$ is:  \n",
        "\n",
        "\\begin{align*}\n",
        "    Q^*((1,2), N) &= 0.7 \\times (0+\\gamma V^*(1,3)) + 0.2\\times(0+\\gamma V^*(1,2)) + 0.1\\times(0+\\gamma V^*(2,2))\\\\\n",
        "    &= \\gamma \\left(0.7\\times V^*(1,3) + 0.2\\times V^*(1,2)+ 0.1\\times V^*(2,2)\\right)\n",
        "\\end{align*}\n",
        "</div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "7lfdKfgxzdLw"
      },
      "source": [
        "**Question**\n",
        "\n",
        "Now you can remark that if we knew the action $\\pi^*((1,2))$ taken by an optimal policy in $(1,2)$, then $Q^*((1,2), \\pi^*(1,2))$ would actually be precisely the optimal long-term return $V^*$ (since it would be the expected return of a policy that acts optimally at every time step, including the first one).\n",
        "\n",
        "\n",
        "Suppose an oracle tells us that $\\pi^*((1,2))=N$. Using the previous exercice, write $V^*(1,2)$ as a function of $V^*(1,3)$, $V^*(1,2)$ and $V^*(2,2)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "5axx0eiBzdLw"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "We have $V^*((1,2)) = Q^*((1,2),N)$, so\n",
        "$$V^*((1,2)) = \\gamma \\left(0.7\\times V^*(1,3) + 0.2\\times V^*(1,2)+ 0.1\\times V^*(2,2)\\right)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <img src=\"https://img.icons8.com/?size=50&id=43167&format=png&color=000000\" style=\"height:30px;display:inline\"> Bellman Evaluation Operator\n",
        "\n",
        "The following cells contain advanced mathematical tools and intuitions to understand the convergence of value-based methods. While this is not a complete proof, the main ideas are presented in a simplified manner to help you grasp the key concepts. For a more rigorous and detailed proof, we encourage you to explore the full explanation provided in this [book](https://github.com/avivt/RL-book-overleaf).\n"
      ],
      "metadata": {
        "id": "S34VKH9x2Fy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Bellman evaluation operator, denoted as $T^\\pi$, helps in evaluating a given policy by updating the value function iteratively until it converges to the true value function for that policy."
      ],
      "metadata": {
        "id": "AgISTzn_NKPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation operator $T^\\pi$ for value functions transforms a function $V: S \\rightarrow \\mathbb{R}$ into:\n",
        "\n",
        "$$\n",
        "T^\\pi V(s) = r(s, \\pi(s)) + \\gamma \\mathbb{E}_{s' \\sim P(s'|s, \\pi(s))} [V(s')]\n",
        "$$\n",
        "\n",
        "This can also be written as:\n",
        "\n",
        "$$\n",
        "T^\\pi V(s) = r(s, \\pi(s)) + \\gamma \\sum_{s' \\in S} P(s'|s, \\pi(s)) V(s')\n",
        "$$\n",
        "\n",
        "Similarly, the evaluation operator $T^\\pi$ for state-action value functions transforms a function $Q: S \\times A \\rightarrow \\mathbb{R}$ into:\n",
        "\n",
        "$$\n",
        "T^\\pi Q(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s' \\sim P(s'|s,a)} [Q^\\pi(s', \\pi(s'))]\n",
        "$$\n",
        "\n",
        "This can also be written as:\n",
        "\n",
        "$$\n",
        "T^\\pi Q(s,a) = r(s,a) + \\gamma \\sum_{s' \\in S} P(s'|s,a) Q^\\pi(s', \\pi(s'))\n",
        "$$"
      ],
      "metadata": {
        "id": "4Tya4rYJ2WYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/task_sign.png?raw=true' width=800/>"
      ],
      "metadata": {
        "id": "rVqiDMtN6Wsa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cvJ-lLIqyCa"
      },
      "source": [
        "#### <img src=\"https://img.icons8.com/?size=50&id=46589&format=png&color=000000\" style=\"height:30px;display:inline\"> Task 2: Bellman operator\n",
        "\n",
        "Write a function that takes an environment, gamma and a state-action value function $Q$ as input and returns the Bellman optimality operator applied to $Q$, $T^* Q$ and the greedy policy with respect to $Q$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "erSy1AfnqyCc"
      },
      "outputs": [],
      "source": [
        "# --------------\n",
        "# Your answer\n",
        "# --------------\n",
        "def bellman_operator(Q, env, gamma=0.9):\n",
        "    \"\"\"\n",
        "    Applies the Bellman operator to the Q-value function.\n",
        "\n",
        "    Parameters:\n",
        "    Q (numpy.ndarray): The current Q-value function.\n",
        "    env (gym.Env): The environment.\n",
        "    gamma (float): The discount factor.\n",
        "\n",
        "    Returns:\n",
        "    TQ (numpy.ndarray): The updated Q-value function after applying the Bellman operator.\n",
        "    greedy_policy (numpy.ndarray): The greedy policy derived from the updated Q-values.\n",
        "    \"\"\"\n",
        "    # Initialize the updated Q-value function\n",
        "    TQ = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "    ###\n",
        "    # To fill\n",
        "    ###\n",
        "    return TQ, greedy_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/solution_sign.png?raw=true' width=800/>"
      ],
      "metadata": {
        "id": "HmVXjArkHZhH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoxjsDAh-5d9"
      },
      "source": [
        "#### <img src=\"https://img.icons8.com/?size=50&id=42816&format=png&color=000000\" style=\"height:30px;display:inline\"> Solution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "# --------------\n",
        "def bellman_operator(Q, env, gamma=0.9):\n",
        "    \"\"\"\n",
        "    Applies the Bellman operator to the Q-value function.\n",
        "\n",
        "    Parameters:\n",
        "    Q (numpy.ndarray): The current Q-value function.\n",
        "    env (gym.Env): The environment.\n",
        "    gamma (float): The discount factor.\n",
        "\n",
        "    Returns:\n",
        "    TQ (numpy.ndarray): The updated Q-value function after applying the Bellman operator.\n",
        "    greedy_policy (numpy.ndarray): The greedy policy derived from the updated Q-values.\n",
        "    \"\"\"\n",
        "    # Initialize the updated Q-value function\n",
        "    TQ = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "    # Iterate over each state and action in the environment\n",
        "    for x in range(env.observation_space.n):\n",
        "        for a in range(env.action_space.n):\n",
        "            # Get the possible outcomes for taking action `a` in state `x`\n",
        "            outcomes = env.unwrapped.P[x][a]\n",
        "            for o in outcomes:\n",
        "                p = o[0]  # Transition probability\n",
        "                y = o[1]  # Next state\n",
        "                r = o[2]  # Reward\n",
        "                TQ[x, a] += p * (r + gamma * np.max(Q[y]))\n",
        "\n",
        "    # Derive the greedy policy from the updated Q-values\n",
        "    greedy_policy = np.argmax(TQ, axis=1)\n",
        "\n",
        "    return TQ, greedy_policy"
      ],
      "metadata": {
        "id": "D_MNzAxi-zlq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Properties of $T^\\pi$\n",
        "\n",
        "1. **Affine Operator**: $T^\\pi$ is an affine operator, defining a linear system of equations.\n",
        "2. **Contraction Mapping**: $T^\\pi$ is a contraction mapping with $\\gamma < 1$, meaning it is a $\\|\\cdot\\|_\\infty$-contraction mapping over the $\\mathcal{F}(S, \\mathbb{R})$ or $\\mathcal{F}(S \\times A, \\mathbb{R})$ Banach space.\n",
        "\n",
        "Due to the contraction property, $V^\\pi$ or $Q^\\pi$ is the unique solution to the fixed point equation $V = T^\\pi V$ or $Q = T^\\pi Q$."
      ],
      "metadata": {
        "id": "CBWl_l0e2xRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we apply $T^\\pi$ enough times, $Q_n$ should converge to $Q^\\pi$, regardless of the initial value $Q_0$.\n",
        "\n",
        "In more formal terms, because $T^\\pi$ is a contraction mapping, the sequence $Q_{n+1} = T^\\pi Q_n$ converges to the fixed point of $T^\\pi$."
      ],
      "metadata": {
        "id": "msHFZRVo3hHB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "GSs1zZhezdLw"
      },
      "source": [
        "Let's compute the sequence $Q_{n+1} = T^\\pi Q_n$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "hidden": true,
        "id": "vs9orHk6zdLx"
      },
      "outputs": [],
      "source": [
        "pi = fl.RIGHT*np.ones((env.observation_space.n), dtype=np.uint8)\n",
        "nb_iter = 20\n",
        "gamma = 0.9\n",
        "\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "Qpi_sequence = [Q]\n",
        "for i in range(nb_iter):\n",
        "    Qnew = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    for x in range(env.observation_space.n):\n",
        "        for a in range(env.action_space.n):\n",
        "            outcomes = env.unwrapped.P[x][a]\n",
        "            for o in outcomes:\n",
        "                p = o[0]\n",
        "                y = o[1]\n",
        "                r = o[2]\n",
        "                Qnew[x,a] += p * (r + gamma * Q[y,pi[y]])\n",
        "    Q = Qnew\n",
        "    Qpi_sequence.append(Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Fnm6huOvzdLx"
      },
      "source": [
        "Let's plot the sequence of $\\| Q_n - Q_{n-1} \\|_\\infty$ to verify the convergence of the sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "hidden": true,
        "id": "ba2cYeJpzdLx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "3440af3a-882d-4ca3-e98a-95c118a1afbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtFklEQVR4nO3dfXRU9b3v8c9MHiYhJAEykBCIBK0KCCQ8lNxovT1qJCIH5djTRvQIpUqrxXvE1HMwVogcW+NDpbQWxVqj7bUWtEdpT2HhxRRslSiVkBafUBQBhQQCkgkJZJKZff/ADI4kIRPI7Id5v9aatWTnt2e+m82sfPzt34PLMAxDAAAAJnGbXQAAAIhthBEAAGAqwggAADAVYQQAAJiKMAIAAExFGAEAAKYijAAAAFMRRgAAgKnizS6gJ4LBoPbu3avU1FS5XC6zywEAAD1gGIaampqUnZ0tt7vr/g9bhJG9e/cqJyfH7DIAAEAv7NmzR8OHD+/y57YII6mpqZKOX0xaWprJ1QAAgJ7w+XzKyckJ/R7vii3CSMejmbS0NMIIAAA2c6ohFgxgBQAApiKMAAAAUxFGAACAqQgjAADAVIQRAABgKsIIAAAwFWEEAACYijACAABMRRgBAACmijiM/OUvf9GMGTOUnZ0tl8ul1atXn/KcjRs3auLEifJ4PPrKV76ip59+uhelAgAAJ4o4jDQ3NysvL0/Lly/vUfudO3dq+vTpuuSSS1RbW6sFCxbopptu0ksvvRRxsQAAwHki3ptm2rRpmjZtWo/br1ixQiNHjtTDDz8sSRo9erReffVV/fSnP1VxcXGkHw8AABymz8eMVFdXq6ioKOxYcXGxqquruzyntbVVPp8v7NUXXv2gQXMqN+tYW6BP3h8AAJxan4eRuro6ZWZmhh3LzMyUz+fT0aNHOz2noqJC6enpoVdOTs4Zr+uoP6Dbn6vVK+8f0EMvbT/j7w8AAHrGkrNpysrK1NjYGHrt2bPnjH9GcmKcHvzGeEnSk6/u1F8/OHDGPwMAAJxan4eRrKws1dfXhx2rr69XWlqakpOTOz3H4/EoLS0t7NUXLhk1RDf8rxGSpDue/7s+a/b3yecAAICu9XkYKSwsVFVVVdix9evXq7CwsK8/ukfuunK0zhmconpfq8pe2CbDMMwuCQCAmBJxGDly5Ihqa2tVW1sr6fjU3draWu3evVvS8Ucss2fPDrW/+eab9dFHH+k///M/9d577+nRRx/Vc889p9tvv/3MXMFpSk6M08+unaCEOJfWvV2n59/8xOySAACIKRGHkTfffFMTJkzQhAkTJEmlpaWaMGGCFi9eLEnat29fKJhI0siRI7VmzRqtX79eeXl5evjhh/WrX/3KUtN6xw5L1w+mni9Juud/3taug80mVwQAQOxwGTZ4LuHz+ZSenq7GxsY+Gz8SCBq67onX9cbOQ5pw1gA9/71CxcdZcnwvAAC20NPf3/y2/Vyc26WlJflKTYrX1t2H9cifd5hdEgAAMYEw8gXDBiTrx/8yTpL0yJ8/0JZdn5lcEQAAzkcY+ZKr8rL1LxOGKWhIt6+q1ZHWdrNLAgDA0QgjnVhy9QUaNiBZuw+16J4/vm12OQAAOBphpBNpSQn6aUm+3C7p91s+0dpt+8wuCQAAxyKMdGHKyEG65Z/OkSSVvbBNdY3HTK4IAABnIox0Y0HReRo/PF2NR9v0g+drFQxafhY0AAC2QxjpRkKcW8tK8pWcEKfXdhxU5Ws7zS4JAADHIYycwtmD+2vRP4+RJD24brve2eszuSIAAJyFMNIDs6bkqGh0pvyBoBas2qpjbQGzSwIAwDEIIz3gcrn0wDfGydvfo/frj+iBde+ZXRIAAI5BGOmhjP4ePfTN8ZKkp177WH95/4DJFQEA4AyEkQhccv4QzSkcIUn6wfN/16Fmv8kVAQBgf4SRCJVdOVrnDumvA02tuvO//yEbbHoMAIClEUYilJQQp2XX5ishzqX/9069Vv1tj9klAQBga4SRXrggO113TD1fkrTkf97RzoZmkysCAMC+CCO9NO/is1V4doaOtgW0YFWt2gJBs0sCAMCWCCO95Ha79PC38pSWFK+/7zmsR6o+MLskAABsiTByGrIHJOu+a8ZJkn6xYYfe/PiQyRUBAGA/hJHT9M/js3XNxGEKGtKCVbVqOtZmdkkAANgKYeQMWHLVBRo+MFmffHZU5X982+xyAACwFcLIGZCalKBlJflyu6QXaj7Vn/6x1+ySAACwDcLIGTI5d5DmX/IVSdJdL2zTvsajJlcEAIA9EEbOoH+/7Fzl5QyQ71i7fvDc3xUMsjorAACnQhg5gxLi3FpWkq/khDht+vCgfvXqR2aXBACA5RFGzrCR3hQtnjFGkvTQS9v19t5GkysCAMDaCCN94Nqv5ujyMZlqCxi6bWWtmlvbzS4JAADLIoz0AZfLpQe+MV5DUj3asf+IFrK7LwAAXSKM9JFBKYl69PqJine79Kd/7FPlax+bXRIAAJZEGOlDk3MH6e7poyVJ9619V298dNDkigAAsB7CSB+bc2GuZuZnKxA0NP/Zrar3HTO7JAAALIUw0sdcLpcqrhmvUVmpajjSqlue2SJ/e9DssgAAsAzCSBQkJ8bp8RsmKTUpXjW7D+tHa94xuyQAACyDMBIlIzJStKwkX5L0m+pdeqHmE3MLAgDAIggjUXTZ6Ez9+2XnSpLKXtjGgmgAAIgwEnULLjtX/3T+YLW2B3XzM1t0uMVvdkkAAJiKMBJlbrdLy0rylTMoWXsOHdWCVbVsqAcAiGmEERMM6JeoFf82SZ54tzZuP6BlVR+YXRIAAKYhjJjkgux0VVwzTpL086oPVPVuvckVAQBgDsKIia6ZOFyzC0dIkm5fVatdB5tNrggAgOgjjJjs7uljNGnEQPmOtet7/3eLjvoDZpcEAEBUEUZMlhjv1qPXT5S3v0fv1TWp7AV2+AUAxBbCiAVkpiVp+XUTFOd2aXXtXv1608dmlwQAQNQQRiyi4OwMlU0bJUn60Zp39bePD5lcEQAA0UEYsZAbvzZS/zx+qNqDhr7/2xrtZ4dfAEAMIIxYiMvl0gPfGK/zMvvrQFOr5j9bo7YAO/wCAJyNMGIxKZ54PX7DZKV64vW3jz/Tj9e8a3ZJAAD0KcKIBY30pmjp5zv8Pr3pY/2h9lNzCwIAoA8RRizq8jGZuvWSr0iSFv73P/TuPp/JFQEA0DcIIxZ2++Xn6X+fN1jH2o7v8Nt4tM3skgAAOOMIIxYW53bpZyX5Gj4wWbsOtqiUHX4BAA5EGLG4gSkndvitem+/frFhh9klAQBwRhFGbGDssHT9aOZYSdJPX35fG7bvN7kiAADOHMKITXxzco6uKzhLhiEtWFmr3QdbzC4JAIAzgjBiI+Uzxig/Z4Aaj7bp5mfY4RcA4AyEERvxxMfpsX+bqIyURL2zz6cfrt7GDr8AANsjjNjM0PRkPXLdBLld0gs1n2rl3/aYXRIAAKelV2Fk+fLlys3NVVJSkgoKCrR58+Zu2y9btkznn3++kpOTlZOTo9tvv13HjrEJXG9deI5XpZefJ0mEEQCA7UUcRlatWqXS0lKVl5erpqZGeXl5Ki4u1v79nc/wePbZZ3XnnXeqvLxc7777rp588kmtWrVKd91112kXH8u+du5gSVJDU6vJlQAAcHoiDiNLly7VvHnzNHfuXI0ZM0YrVqxQv379VFlZ2Wn7TZs26aKLLtJ1112n3NxcTZ06VbNmzTplbwq6l5GSKElqONLKuBEAgK1FFEb8fr+2bNmioqKiE2/gdquoqEjV1dWdnnPhhRdqy5YtofDx0Ucfae3atbryyiu7/JzW1lb5fL6wF8J5+3skSa3tQR1pbTe5GgAAei8+ksYNDQ0KBALKzMwMO56Zman33nuv03Ouu+46NTQ06Gtf+5oMw1B7e7tuvvnmbh/TVFRUaMmSJZGUFnOSE+OUkhinZn9AB4/4lZqUYHZJAAD0Sp/Pptm4caPuu+8+Pfroo6qpqdELL7ygNWvW6N577+3ynLKyMjU2NoZee/YwSLMzGZ/3jhxsZtwIAMC+IuoZ8Xq9iouLU319fdjx+vp6ZWVldXrOokWLdMMNN+imm26SJI0bN07Nzc367ne/qx/+8Idyu0/OQx6PRx6PJ5LSYlJG/0TtPtSiA01+s0sBAKDXIuoZSUxM1KRJk1RVVRU6FgwGVVVVpcLCwk7PaWlpOSlwxMXFSRIDL0+Tl54RAIADRNQzIkmlpaWaM2eOJk+erClTpmjZsmVqbm7W3LlzJUmzZ8/WsGHDVFFRIUmaMWOGli5dqgkTJqigoEA7duzQokWLNGPGjFAoQe94+x+fUXPwCD0jAAD7ijiMlJSU6MCBA1q8eLHq6uqUn5+vdevWhQa17t69O6wn5O6775bL5dLdd9+tTz/9VIMHD9aMGTP04x//+MxdRYzKSDneM9JwhJ4RAIB9uQwbPCvx+XxKT09XY2Oj0tLSzC7HMp5+bafu+Z93NH3cUC2/fqLZ5QAAEKanv7/Zm8bGOmbT0DMCALAzwoiNZfQ/sQorAAB2RRixscGh2TQMYAUA2BdhxMY6HtMcbmlTWyBocjUAAPQOYcTGBiQnKM7tkiQdoncEAGBThBEbc7tdGpTCuBEAgL0RRmwuI4WFzwAA9kYYsTkv03sBADZHGLE5loQHANgdYcTmQgufsVkeAMCmCCM2F1r4rImeEQCAPRFGbM4bWviMnhEAgD0RRmyOMSMAALsjjNhcRsrnPSPMpgEA2BRhxOZObJbnl2EYJlcDAEDkCCM21zFmxB8Iqqm13eRqAACIHGHE5pIS4tTfEy+JcSMAAHsijDjAiUc1jBsBANgPYcQBQtN7CSMAABsijDhARsqJQawAANgNYcQBMtgsDwBgY4QRBxjMwmcAABsjjDhABkvCAwBsjDDiAGyWBwCwM8KIA3TMpmmgZwQAYEOEEQdgszwAgJ0RRhygY7O8xqNt8rcHTa4GAIDIEEYcID05QfFulyTpUDO9IwAAeyGMOIDb7dKgFJaEBwDYE2HEIVj4DABgV4QRh2AQKwDArggjDuFl4TMAgE0RRhyCzfIAAHZFGHEIbypjRgAA9kQYcYiOnhHGjAAA7IYw4hBeZtMAAGyKMOIQoQGs9IwAAGyGMOIQHTv3HmxulWEYJlcDAEDPEUYcomMF1raAId/RdpOrAQCg5wgjDpGUEKfUpHhJUgNrjQAAbIQw4iCMGwEA2BFhxEEy2CwPAGBDhBEHCQ1iJYwAAGyEMOIgJ9Ya4TENAMA+CCMOksFmeQAAGyKMOIj388c0DU30jAAA7IMw4iBeekYAADZEGHEQNssDANgRYcRBOsaMHGA2DQDARggjDjL48zDSdKxdre0Bk6sBAKBnCCMOkpYcr3i3S5J0qJlHNQAAeyCMOIjL5QotfMaMGgCAXRBGHCa08BkzagAANkEYcZgMNssDANgMYcRhvGyWBwCwGcKIw3hTO3pGCCMAAHvoVRhZvny5cnNzlZSUpIKCAm3evLnb9ocPH9b8+fM1dOhQeTwenXfeeVq7dm2vCkb3WPgMAGA38ZGesGrVKpWWlmrFihUqKCjQsmXLVFxcrO3bt2vIkCEntff7/br88ss1ZMgQ/f73v9ewYcO0a9cuDRgw4EzUjy9h4TMAgN1EHEaWLl2qefPmae7cuZKkFStWaM2aNaqsrNSdd955UvvKykodOnRImzZtUkJCgiQpNzf39KpGlzo2y6NnBABgFxE9pvH7/dqyZYuKiopOvIHbraKiIlVXV3d6zh//+EcVFhZq/vz5yszM1NixY3XfffcpEOh6hdDW1lb5fL6wF3qGzfIAAHYTURhpaGhQIBBQZmZm2PHMzEzV1dV1es5HH32k3//+9woEAlq7dq0WLVqkhx9+WD/60Y+6/JyKigqlp6eHXjk5OZGUGdMyvtAzEgwaJlcDAMCp9flsmmAwqCFDhuiXv/ylJk2apJKSEv3whz/UihUrujynrKxMjY2NodeePXv6ukzHyEg53jPSHjTkO9ZmcjUAAJxaRGNGvF6v4uLiVF9fH3a8vr5eWVlZnZ4zdOhQJSQkKC4uLnRs9OjRqqurk9/vV2Ji4knneDweeTyeSErD5xLj3UpLipfvWLsajvg1oN/Jf78AAFhJRD0jiYmJmjRpkqqqqkLHgsGgqqqqVFhY2Ok5F110kXbs2KFgMBg69v7772vo0KGdBhGcvtCS8MyoAQDYQMSPaUpLS/XEE0/o17/+td59913dcsstam5uDs2umT17tsrKykLtb7nlFh06dEi33Xab3n//fa1Zs0b33Xef5s+ff+auAmG8LAkPALCRiKf2lpSU6MCBA1q8eLHq6uqUn5+vdevWhQa17t69W273iYyTk5Ojl156SbfffrvGjx+vYcOG6bbbbtPChQvP3FUgTGgQKzNqAAA24DIMw/JTLnw+n9LT09XY2Ki0tDSzy7G8u1dv0zOv79a/X/oVlU493+xyAAAxqqe/v9mbxoFCY0aaeUwDALA+wogDZfRnszwAgH0QRhzI+/lmeQ0MYAUA2ABhxIHoGQEA2AlhxIHYLA8AYCeEEQfq6Blpam3XsbauNyQEAMAKCCMOlJYUr8S447f2IDNqAAAWRxhxIJfL9YXdexk3AgCwNsKIQ2UwbgQAYBOEEYfKSDk+buQAPSMAAIsjjDgUm+UBAOyCMOJQXsaMAABsgjDiUB1jRhoIIwAAiyOMOFToMQ1TewEAFkcYcaiOhc/YnwYAYHWEEYfKSOExDQDAHggjDjU49XjPyKFmv4JBw+RqAADoGmHEoQb2O94zEggaajzaZnI1AAB0jTDiUInxbqUnJ0jiUQ0AwNoIIw7mDU3vZRArAMC6CCMOlhGa3kvPCADAuggjDhbqGWkijAAArIsw4mAsfAYAsAPCiIN17NzLmBEAgJURRhyM/WkAAHZAGHGw0GMawggAwMIIIw7WMYCVMSMAACsjjDhYaLM8ZtMAACyMMOJgHT0jzf6AjvoDJlcDAEDnCCMO1t8Tr8T447eYhc8AAFZFGHEwl8slbwpLwgMArI0w4nAZzKgBAFgcYcThQjNq6BkBAFgUYcThOnpGDtAzAgCwKMKIw2XQMwIAsDjCiMMNDm2WR88IAMCaCCMOR88IAMDqCCMOd2LnXnpGAADWRBhxuI7N8lhnBABgVYQRh+uY2nuouVXBoGFyNQAAnIww4nADP1+BNWhIn7XQOwIAsB7CiMMlxLk1sF+CJOlgM2EEAGA9hJEYkNGfQawAAOsijMSADDbLAwBYGGEkBnhT2SwPAGBdhJEY4E1h4TMAgHURRmIAY0YAAFZGGIkBLHwGALAywkgMCO1Pw2Z5AAALIozEgI5VWHlMAwCwIsJIDOh4TMMAVgCAFRFGYkDHANYWf0At/naTqwEAIBxhJAakJMbJE3/8VtM7AgCwGsJIDHC5XF+YUcO4EQCAtRBGYkTHIFZ6RgAAVkMYiREsfAYAsKpehZHly5crNzdXSUlJKigo0ObNm3t03sqVK+VyuTRz5szefCxOQ6hnpJmeEQCAtUQcRlatWqXS0lKVl5erpqZGeXl5Ki4u1v79+7s97+OPP9Ydd9yhiy++uNfFovfoGQEAWFXEYWTp0qWaN2+e5s6dqzFjxmjFihXq16+fKisruzwnEAjo+uuv15IlS3T22WefVsHonYyUjoXP6BkBAFhLRGHE7/dry5YtKioqOvEGbreKiopUXV3d5Xn/9V//pSFDhujGG2/s0ee0trbK5/OFvXB6Tix8Rs8IAMBaIgojDQ0NCgQCyszMDDuemZmpurq6Ts959dVX9eSTT+qJJ57o8edUVFQoPT099MrJyYmkTHSCVVgBAFbVp7NpmpqadMMNN+iJJ56Q1+vt8XllZWVqbGwMvfbs2dOHVcaGDPanAQBYVHwkjb1er+Li4lRfXx92vL6+XllZWSe1//DDD/Xxxx9rxowZoWPBYPD4B8fHa/v27TrnnHNOOs/j8cjj8URSGk6hI4wcavErEDQU53aZXBEAAMdF1DOSmJioSZMmqaqqKnQsGAyqqqpKhYWFJ7UfNWqUtm3bptra2tDrqquu0iWXXKLa2loev0TRoH6Jcrkkw5A+a+FRDQDAOiLqGZGk0tJSzZkzR5MnT9aUKVO0bNkyNTc3a+7cuZKk2bNna9iwYaqoqFBSUpLGjh0bdv6AAQMk6aTj6FvxcW4N7JeoQ81+NRxpDY0hAQDAbBGHkZKSEh04cECLFy9WXV2d8vPztW7dutCg1t27d8vtZmFXK8pIOR5GGMQKALASl2EYhtlFnIrP51N6eroaGxuVlpZmdjm2NeuXr6v6o4P62bX5ujp/mNnlAAAcrqe/v+nCiCEZbJYHALAgwkgM8bIkPADAgggjMcRLzwgAwIIIIzGkY7O8g830jAAArIMwEkM6Nss7QM8IAMBCCCMxxJvKZnkAAOshjMQQbwqb5QEArIcwEkM6pvYebQuoubXd5GoAADiOMBJDUjzxSk6Ik0TvCADAOggjMaajd6SBGTUAAIsgjMSYjum9DU2EEQCANRBGYszgjoXPmnlMAwCwBsJIjMlIYXovAMBaCCMxJjRmhAGsAACLIIzEGDbLAwBYDWEkxmSwWR4AwGIIIzGGnhEAgNUQRmKMN7RzLz0jAABrIIzEmI7HNJ+1+NUeCJpcDQAAhJGYM7BfolwuyTCkQy30jgAAzEcYiTFxbpcG9WMQKwDAOggjMSg0boQwAgCwAMJIDDqx8BkzagAA5iOMxKAMpvcCACyEMBKDvGyWBwCwEMJIDAotfNZEzwgAwHyEkRiUkULPCADAOggjMejEbBp6RgAA5iOMxKATs2noGQEAmI8wEoO+uFmeYRgmVwMAiHWEkRjU0TPS2h5Usz9gcjUAgFhHGIlB/RLj1S8xThLjRgAA5iOMxChWYQUAWAVhJEadGDfCIFYAgLkIIzEqI4XN8gAA1kAYiVFeHtMAACyCMBKjWPgMAGAVhJEYFRrAypLwAACTEUZiVAab5QEALIIwEqM6xoywWR4AwGyEkRjFmBEAgFUQRmJURsrxnpHPWtrUFgiaXA0AIJYRRmLUwH6JcruO//dnPKoBAJiIMBKj3G6XBqWwCisAwHyEkRjGwmcAACsgjMSw0CDWZsIIAMA8hJEY1rHwGfvTAADMRBiJYR2b5R3gMQ0AwESEkRhGzwgAwAoIIzFsMAufAQAsgDASw0Kb5dEzAgAwEWEkhmXQMwIAsADCSAwLrTPS7JdhGCZXAwCIVYSRGNYxm8bfHlRTa7vJ1QAAYhVhJIYlJ8YpJTFOEjNqAADmIYzEOG8q40YAAObqVRhZvny5cnNzlZSUpIKCAm3evLnLtk888YQuvvhiDRw4UAMHDlRRUVG37RFdGSnsTwMAMFfEYWTVqlUqLS1VeXm5ampqlJeXp+LiYu3fv7/T9hs3btSsWbO0YcMGVVdXKycnR1OnTtWnn3562sXj9HXMqGF6LwDALBGHkaVLl2revHmaO3euxowZoxUrVqhfv36qrKzstP1vf/tbff/731d+fr5GjRqlX/3qVwoGg6qqqjrt4nH6QpvlEUYAACaJKIz4/X5t2bJFRUVFJ97A7VZRUZGqq6t79B4tLS1qa2vToEGDumzT2toqn88X9kLf6Jjey869AACzRBRGGhoaFAgElJmZGXY8MzNTdXV1PXqPhQsXKjs7OyzQfFlFRYXS09NDr5ycnEjKRAQYMwIAMFtUZ9Pcf//9WrlypV588UUlJSV12a6srEyNjY2h1549e6JYZWzpmE3DmBEAgFniI2ns9XoVFxen+vr6sOP19fXKysrq9tyf/OQnuv/++/Xyyy9r/Pjx3bb1eDzyeDyRlIZe6lj4jKm9AACzRNQzkpiYqEmTJoUNPu0YjFpYWNjleQ8++KDuvfderVu3TpMnT+59tTjjvGyWBwAwWUQ9I5JUWlqqOXPmaPLkyZoyZYqWLVum5uZmzZ07V5I0e/ZsDRs2TBUVFZKkBx54QIsXL9azzz6r3Nzc0NiS/v37q3///mfwUtAbHbNpGo+2yd8eVGI86+ABAKIr4jBSUlKiAwcOaPHixaqrq1N+fr7WrVsXGtS6e/duud0nfqE99thj8vv9+td//dew9ykvL9c999xzetXjtKUnJyjO7VIgaOizFr8y07oeywMAQF9wGTbYrtXn8yk9PV2NjY1KS0szuxzH+eqPX9aBplb96f98TWOHpZtdDgDAIXr6+5s+eZxY+KyZcSMAgOgjjODEwmfMqAEAmIAwAhY+AwCYijAC9qcBAJiKMAJ27gUAmIowAmX05zENAMA8hBFocGg2DWEEABB9hBGEekYYMwIAMANhBKExIweP+GWDNfAAAA5DGEFoaq8/EJTvWLvJ1QAAYg1hBEpKiFOq5/g2RSx8BgCINsIIJH1xRg3jRgAA0UUYgaQvjhuhZwQAEF2EEUg6sT9NA5vlAQCijDACSV9YhbWJnhEAQHQRRiBJ8n4+o4aFzwAA0UYYgSTJm8pmeQAAcxBGIEnKSOnYLI+eEQBAdBFGIIkl4QEA5iGMQJLk7U/PCADAHIQRSDoxtdd3rF3+9qDJ1QAAYglhBJKktKQExbtdkphRAwCILsIIJElut4txIwAAUxBGEMKMGgCAGQgjCGGzPACAGQgjCBnMZnkAABMQRhASGjPCZnkAgCgijCCEzfIAAGYgjCAktPAZPSMAgCgijCDkxNReekYAANFDGEGIl6m9AAATEEYQ4k09seiZYRgmVwMAiBWEEYQMSjkeRtqDhnxH202uBgAQKwgjCPHExyk1KV6SdIBHNQCAKCGMIAwLnwEAoo0wgjAsfAYAiDbCCMKwWR4AINoIIwjDZnkAgGgjjCCMlzEjAIAoI4wgjDfUM0IYAQBEB2EEYTJCPSM8pgEARAdhBGFCj2mYTQMAiBLCCMKEBrA28ZgGABAdhBGE6dgsr6m1XcfaAiZXAwCIBYQRhElLjldCnEuSdIhHNQCAKCCMIIzL5WLhMwBAVBFGcJLQkvDMqAEARAFhBCfpmFFDzwgAIBoIIzgJS8IDAKKJMIKTsCQ8ACCaCCM4SceS8Cx8BgCIBsIITsJsGgBANBFGcBLGjAAAookwgpMwZgQAEE2EEZykI4wcavYrGDRMrgYA4HSEEZxkUMrxxzTtQUONR9tMrgYA4HS9CiPLly9Xbm6ukpKSVFBQoM2bN3fb/vnnn9eoUaOUlJSkcePGae3atb0qFtGRGO9WenKCJOlgM49qAAB9K+IwsmrVKpWWlqq8vFw1NTXKy8tTcXGx9u/f32n7TZs2adasWbrxxhu1detWzZw5UzNnztRbb7112sWj7zCIFQAQLRGHkaVLl2revHmaO3euxowZoxUrVqhfv36qrKzstP3PfvYzXXHFFfqP//gPjR49Wvfee68mTpyoX/ziF6ddPPqOl+m9AIAoiY+ksd/v15YtW1RWVhY65na7VVRUpOrq6k7Pqa6uVmlpadix4uJirV69usvPaW1tVWvriV+CPp8vkjJxBnhTj/eMPPvGbm3Z9ZnJ1QAA+tp3LhqpnEH9TPnsiMJIQ0ODAoGAMjMzw45nZmbqvffe6/Scurq6TtvX1dV1+TkVFRVasmRJJKXhDBs+8Pg/yE0fHtSmDw+aXA0AoK/NyMu2RxiJlrKysrDeFJ/Pp5ycHBMrij3f/d9nKy0pXkfbAmaXAgCIgsy0JNM+O6Iw4vV6FRcXp/r6+rDj9fX1ysrK6vScrKysiNpLksfjkcfjiaQ0nGHe/h7deum5ZpcBAIgBEQ1gTUxM1KRJk1RVVRU6FgwGVVVVpcLCwk7PKSwsDGsvSevXr++yPQAAiC0RP6YpLS3VnDlzNHnyZE2ZMkXLli1Tc3Oz5s6dK0maPXu2hg0bpoqKCknSbbfdpq9//et6+OGHNX36dK1cuVJvvvmmfvnLX57ZKwEAALYUcRgpKSnRgQMHtHjxYtXV1Sk/P1/r1q0LDVLdvXu33O4THS4XXnihnn32Wd1999266667dO6552r16tUaO3bsmbsKAABgWy7DMCy/+YjP51N6eroaGxuVlpZmdjkAAKAHevr7m71pAACAqQgjAADAVIQRAABgKsIIAAAwFWEEAACYijACAABMRRgBAACmIowAAABTEUYAAICpIl4O3gwdi8T6fD6TKwEAAD3V8Xv7VIu92yKMNDU1SZJycnJMrgQAAESqqalJ6enpXf7cFnvTBINB7d27V6mpqXK5XGfsfX0+n3JycrRnz56Y2PMmlq6Xa3WuWLpertW5YuV6DcNQU1OTsrOzwzbR/TJb9Iy43W4NHz68z94/LS3N0f8YviyWrpdrda5Yul6u1bli4Xq76xHpwABWAABgKsIIAAAwVUyHEY/Ho/Lycnk8HrNLiYpYul6u1bli6Xq5VueKtes9FVsMYAUAAM4V0z0jAADAfIQRAABgKsIIAAAwFWEEAACYyvFhZPny5crNzVVSUpIKCgq0efPmbts///zzGjVqlJKSkjRu3DitXbs2SpWenoqKCn31q19VamqqhgwZopkzZ2r79u3dnvP000/L5XKFvZKSkqJUce/dc889J9U9atSobs+x633Nzc096VpdLpfmz5/faXu73dO//OUvmjFjhrKzs+VyubR69eqwnxuGocWLF2vo0KFKTk5WUVGRPvjgg1O+b6Tf+2jo7lrb2tq0cOFCjRs3TikpKcrOztbs2bO1d+/ebt+zN9+FaDjVff32t799Ut1XXHHFKd/XivdVOvX1dvYddrlceuihh7p8T6ve277i6DCyatUqlZaWqry8XDU1NcrLy1NxcbH279/faftNmzZp1qxZuvHGG7V161bNnDlTM2fO1FtvvRXlyiP3yiuvaP78+Xr99de1fv16tbW1aerUqWpubu72vLS0NO3bty/02rVrV5QqPj0XXHBBWN2vvvpql23tfF//9re/hV3n+vXrJUnf/OY3uzzHTve0ublZeXl5Wr58eac/f/DBB/Xzn/9cK1as0BtvvKGUlBQVFxfr2LFjXb5npN/7aOnuWltaWlRTU6NFixappqZGL7zwgrZv366rrrrqlO8byXchWk51XyXpiiuuCKv7d7/7XbfvadX7Kp36er94nfv27VNlZaVcLpe+8Y1vdPu+Vry3fcZwsClTphjz588P/TkQCBjZ2dlGRUVFp+2/9a1vGdOnTw87VlBQYHzve9/r0zr7wv79+w1JxiuvvNJlm6eeespIT0+PXlFnSHl5uZGXl9fj9k66r7fddptxzjnnGMFgsNOf2/WeGoZhSDJefPHF0J+DwaCRlZVlPPTQQ6Fjhw8fNjwej/G73/2uy/eJ9Htvhi9fa2c2b95sSDJ27drVZZtIvwtm6Oxa58yZY1x99dURvY8d7qth9OzeXn311call17abRs73NszybE9I36/X1u2bFFRUVHomNvtVlFRkaqrqzs9p7q6Oqy9JBUXF3fZ3soaGxslSYMGDeq23ZEjRzRixAjl5OTo6quv1ttvvx2N8k7bBx98oOzsbJ199tm6/vrrtXv37i7bOuW++v1+PfPMM/rOd77T7YaRdr2nX7Zz507V1dWF3bv09HQVFBR0ee968723qsbGRrlcLg0YMKDbdpF8F6xk48aNGjJkiM4//3zdcsstOnjwYJdtnXRf6+vrtWbNGt14442nbGvXe9sbjg0jDQ0NCgQCyszMDDuemZmpurq6Ts+pq6uLqL1VBYNBLViwQBdddJHGjh3bZbvzzz9flZWV+sMf/qBnnnlGwWBQF154oT755JMoVhu5goICPf3001q3bp0ee+wx7dy5UxdffLGampo6be+U+7p69WodPnxY3/72t7tsY9d72pmO+xPJvevN996Kjh07poULF2rWrFndbqIW6XfBKq644gr95je/UVVVlR544AG98sormjZtmgKBQKftnXJfJenXv/61UlNTdc0113Tbzq73trdssWsvIjN//ny99dZbp3y+WFhYqMLCwtCfL7zwQo0ePVqPP/647r333r4us9emTZsW+u/x48eroKBAI0aM0HPPPdej/9uwqyeffFLTpk1TdnZ2l23sek9xQltbm771rW/JMAw99thj3ba163fh2muvDf33uHHjNH78eJ1zzjnauHGjLrvsMhMr63uVlZW6/vrrTzmw3K73trcc2zPi9XoVFxen+vr6sOP19fXKysrq9JysrKyI2lvRrbfeqj/96U/asGGDhg8fHtG5CQkJmjBhgnbs2NFH1fWNAQMG6Lzzzuuybifc1127dunll1/WTTfdFNF5dr2nkkL3J5J715vvvZV0BJFdu3Zp/fr1EW8tf6rvglWdffbZ8nq9XdZt9/va4a9//au2b98e8fdYsu+97SnHhpHExERNmjRJVVVVoWPBYFBVVVVh/+f4RYWFhWHtJWn9+vVdtrcSwzB066236sUXX9Sf//xnjRw5MuL3CAQC2rZtm4YOHdoHFfadI0eO6MMPP+yybjvf1w5PPfWUhgwZounTp0d0nl3vqSSNHDlSWVlZYffO5/PpjTfe6PLe9eZ7bxUdQeSDDz7Qyy+/rIyMjIjf41TfBav65JNPdPDgwS7rtvN9/aInn3xSkyZNUl5eXsTn2vXe9pjZI2j70sqVKw2Px2M8/fTTxjvvvGN897vfNQYMGGDU1dUZhmEYN9xwg3HnnXeG2r/22mtGfHy88ZOf/MR49913jfLyciMhIcHYtm2bWZfQY7fccouRnp5ubNy40di3b1/o1dLSEmrz5etdsmSJ8dJLLxkffvihsWXLFuPaa681kpKSjLffftuMS+ixH/zgB8bGjRuNnTt3Gq+99ppRVFRkeL1eY//+/YZhOOu+GsbxWQNnnXWWsXDhwpN+Zvd72tTUZGzdutXYunWrIclYunSpsXXr1tAMkvvvv98YMGCA8Yc//MH4xz/+YVx99dXGyJEjjaNHj4be49JLLzUeeeSR0J9P9b03S3fX6vf7jauuusoYPny4UVtbG/Ydbm1tDb3Hl6/1VN8Fs3R3rU1NTcYdd9xhVFdXGzt37jRefvllY+LEica5555rHDt2LPQedrmvhnHqf8eGYRiNjY1Gv379jMcee6zT97DLve0rjg4jhmEYjzzyiHHWWWcZiYmJxpQpU4zXX3899LOvf/3rxpw5c8LaP/fcc8Z5551nJCYmGhdccIGxZs2aKFfcO5I6fT311FOhNl++3gULFoT+bjIzM40rr7zSqKmpiX7xESopKTGGDh1qJCYmGsOGDTNKSkqMHTt2hH7upPtqGIbx0ksvGZKM7du3n/Qzu9/TDRs2dPrvtuOagsGgsWjRIiMzM9PweDzGZZdddtLfw4gRI4zy8vKwY919783S3bXu3Lmzy+/whg0bQu/x5Ws91XfBLN1da0tLizF16lRj8ODBRkJCgjFixAhj3rx5J4UKu9xXwzj1v2PDMIzHH3/cSE5ONg4fPtzpe9jl3vYVl2EYRp92vQAAAHTDsWNGAACAPRBGAACAqQgjAADAVIQRAABgKsIIAAAwFWEEAACYijACAABMRRgBAACmIowAAABTEUYAAICpCCMAAMBUhBEAAGCq/w/FGSy0GINUoAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "residuals = []\n",
        "for i in range(1, len(Qpi_sequence)):\n",
        "    residuals.append(np.max(np.abs(Qpi_sequence[i]-Qpi_sequence[i-1])))\n",
        "\n",
        "plt.plot(residuals)\n",
        "plt.figure()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "tAKXdNMdzdLx"
      },
      "source": [
        "### Bellman optimality operator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "s8aD8SW2zdLx"
      },
      "source": [
        "We can unfold the same kind of reasoning on the value of an optimal policy. We write:\n",
        "$$V^{\\pi^*} = V^*, \\quad Q^{\\pi^*} = Q^*$$\n",
        "\n",
        "\n",
        "Any policy $\\pi$ defined by $\\pi(s) \\in \\arg\\max\\limits_{a\\in A} Q^*(s,a)$ is an optimal policy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "1euKAukCzdLx"
      },
      "source": [
        "The optimal value function obeys:\n",
        "\\begin{align*}\n",
        "    V^*(s) &= \\max\\limits_{a\\in A} \\left[ r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} V^*(s') \\right]\\\\\n",
        "        &= \\max\\limits_{a\\in A} \\left[ r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a) V^*(s') \\right]\n",
        "\\end{align*}\n",
        "or in terms of $Q$-functions:\n",
        "\\begin{align*}\n",
        "    Q^*(s,a) &= r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ \\max_{a'\\in A} Q^*(s',a') \\right]\\\\\n",
        "        &= r(s,a) + \\gamma \\sum\\limits_{s'\\in S}p(s'|s,a) \\max\\limits_{a'\\in A} Q^*(s',a')\n",
        "\\end{align*}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "rPwCr3TazdLx"
      },
      "source": [
        "We have also defined the **Bellman optimality operator $T^*$** (on $V$ and $Q$ functions) as:\n",
        "\n",
        "$$\\left(T^*V\\right)(s) = \\max\\limits_{a\\in A} \\left[ r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} V(s') \\right]$$\n",
        "$$\\left(T^*Q\\right)(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ \\max_{a'\\in A} Q(s',a') \\right]$$\n",
        "\n",
        "\n",
        "So finding $V^*$ (resp. $Q^*$) boils down to solving $V= T^* V$ (resp. $Q = T^* Q$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "KwaEW70bzdLx"
      },
      "source": [
        "#### Properties of $T^*$\n",
        "<ol>\n",
        "<li> $T^*$ is non-linear.<br>\n",
        "<li> $T^*$ is a contraction mapping<br>\n",
        "With $\\gamma<1$, $T^*$ is a $\\| \\cdot \\|_\\infty$-contraction mapping over the $\\mathcal{F}(S,\\mathbb{R})$ (resp. $\\mathcal{F}(S\\times A,\\mathbb{R})$) Banach space.<br>\n",
        "$\\Rightarrow$ With $\\gamma<1$, $V^*$ (resp. $Q^*$) is the unique solution to the fixed point equation:<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V=T^* V$ (resp. $Q=T^* Q$).\n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "97pSqXMZzdLx"
      },
      "source": [
        "### Dynamic Programming for the optimality equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "6AESAZ6azdLx"
      },
      "source": [
        "Repeatedly applying $T^*$ to an initial function $Q_0$ yields the sequence $Q_{n+1} = T^* Q_n$ that converges to $Q^*$.\n",
        "\n",
        "The implementation of this sequence's computation is the algorithm called **Value Iteration**.\n",
        "\n",
        "Let's compute the sequence $Q_{n+1} = T^* Q_n$.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "hidden": true,
        "id": "fLjLXGK4zdLy"
      },
      "outputs": [],
      "source": [
        "gamma = 0.9\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "Qopt_sequence = [Q]\n",
        "for i in range(nb_iter):\n",
        "    Qnew = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    for x in range(env.observation_space.n):\n",
        "        for a in range(env.action_space.n):\n",
        "            outcomes = env.unwrapped.P[x][a]\n",
        "            for o in outcomes:\n",
        "                p = o[0]\n",
        "                y = o[1]\n",
        "                r = o[2]\n",
        "                Qnew[x,a] += p * (r + np.max(Q[y,:]) )\n",
        "    Q = Qnew\n",
        "    Qopt_sequence.append(Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "SF0DdqYyzdLy"
      },
      "source": [
        "\n",
        "Let's plot the sequence of $\\| Q_n - Q_{n-1} \\|_\\infty$ to verify the convergence of the sequence.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "hidden": true,
        "id": "JCCUd_3rzdLy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "f8531d0d-d3b1-472e-f965-e84a0f23fc62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "residuals = []\n",
        "for i in range(1, len(Qopt_sequence)):\n",
        "    residuals.append(np.max(np.abs(Qopt_sequence[i]-Qopt_sequence[i-1])))\n",
        "\n",
        "plt.plot(residuals)\n",
        "plt.figure()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=104299&format=png&color=000000\" style=\"height:30px;display:inline\"> Value Iteration\n",
        "\n",
        "\n",
        "\n",
        "Value iteration is a method used in dynamic programming to compute the optimal value function $V^*(s)$ and derive the optimal policy $\\pi^*$. This method is particularly useful when the reward function and transition probabilities are known, and the state and action spaces are not excessively large.\n"
      ],
      "metadata": {
        "id": "MVqn4Kc8Aomo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dynamic Programming\n",
        "\n",
        "To understand value iteration, let's assume we know the transition probabilities $P(s'|s,a)$ and that both state ($s$) and action ($a$) spaces are small and discrete. This is known as the \"known dynamics\" setting, which is not typical in model-free reinforcement learning but is useful for deriving the basic algorithm.\n",
        "\n",
        "In this setting, we can represent the entire state-action space using a table. For instance, in a grid world with 16 states and 4 possible actions (left, right, up, down), we can store the value function $V^\\pi(s)$ in a table with 16 entries."
      ],
      "metadata": {
        "id": "IjyHFajTAwtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Value Iteration Algorithm\n",
        "\n",
        "Value iteration simplifies the policy iteration algorithm by directly computing the optimal value function without explicitly maintaining a policy. The algorithm consists of the following steps:\n",
        "\n",
        "1. **Initialize** $Q_0(s,a) = 0$ for all $(s,a)$.\n",
        "2. **Iterate**: For each $n$:\n",
        "   - Compute the new Q-values:\n",
        "\n",
        "     $$\n",
        "     Q_{n+1}(s,a) = \\sum_{s'} P(s'|s,a) \\left[ r(s,a,s') + \\gamma \\max_{a'} Q_n(s',a') \\right]\n",
        "     $$\n",
        "\n",
        "3. **Convergence**: Continue iterating until $Q_n$ converges to $Q^*$."
      ],
      "metadata": {
        "id": "r2dwoOb2A6TT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Once $Q^*(s,a)$ is computed, the optimal policy $\\pi^*$ can be derived as:\n",
        "\n",
        "$$\n",
        "\\pi^*(s) = \\arg \\max_{a \\in A} Q^*(s,a)\n",
        "$$"
      ],
      "metadata": {
        "id": "kwUt073cA_E4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/task_sign.png?raw=true' width=800/>"
      ],
      "metadata": {
        "id": "m7emukRMA1_8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AL859ElqyCf"
      },
      "source": [
        "#### <img src=\"https://img.icons8.com/?size=50&id=46589&format=png&color=000000\" style=\"height:30px;display:inline\"> Task 3: Value iteration\n",
        "\n",
        "1. (Optimal Value function) Write a function that takes as input an initial state-action value function `Q0` and an environment `env` and returns a vector `Q` such that $||T^* Q -  Q ||_\\infty \\leq \\varepsilon $ and the greedy policy with respect to $Q$.\n",
        "2. Test the convergence of the function you implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "57Y249ICqyCf"
      },
      "outputs": [],
      "source": [
        "# --------------\n",
        "# Your answer to 1.\n",
        "# --------------\n",
        "def value_iteration(Q0, env, epsilon=1e-5):\n",
        "    \"\"\"\n",
        "    Finding the optimal value function. To be done!\n",
        "    \"\"\"\n",
        "    TQ = 0\n",
        "    greedy_policy = []\n",
        "    return TQ, greedy_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "vKffKW-DqyCg"
      },
      "outputs": [],
      "source": [
        "# --------------\n",
        "# Your answer to 2.\n",
        "# --------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/solution_sign.png?raw=true' width=800/>"
      ],
      "metadata": {
        "id": "ugsyvNiHHfa-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67OdoKL63eDD"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/?size=50&id=42816&format=png&color=000000\" style=\"height:30px;display:inline\"> Solution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------\n",
        "# Solution to 1.\n",
        "# --------------\n",
        "def value_iteration(Q0, env, epsilon=1e-5):\n",
        "    \"\"\"\n",
        "    Finding the optimal value function. To be done!\n",
        "    \"\"\"\n",
        "    Q = Q0\n",
        "    while True:\n",
        "        TQ, greedy_policy = bellman_operator(Q, env)\n",
        "\n",
        "        err = np.abs(TQ-Q).max()\n",
        "        if err < epsilon:\n",
        "            return TQ, greedy_policy\n",
        "\n",
        "        Q = TQ"
      ],
      "metadata": {
        "id": "Otv6VQTD-MB3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------\n",
        "# Solution to 2.\n",
        "# --------------\n",
        "epsilon = 1e-6\n",
        "Q0 = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "Q, greedy_policy = value_iteration(Q0, env, epsilon)\n",
        "err = np.abs(Q - bellman_operator(Q, env)[0]).max()\n",
        "print(\"norm of T(Q) - Q = \", err)\n",
        "assert err <= epsilon"
      ],
      "metadata": {
        "id": "e8INXbf9-NpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4f1cb8f-caec-4381-8034-342a164b6b77"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "norm of T(Q) - Q =  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<img src=\"https://img.icons8.com/?size=50&id=qjEfYyGK_vq-&format=png&color=000000\" style=\"height:30px;display:inline\">  Monte Carlo vs Temporal Difference Learning"
      ],
      "metadata": {
        "id": "sJBNkA_h64g1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In reinforcement learning, Monte Carlo (MC) and Temporal Difference (TD) are two fundamental strategies used to train value functions or policy functions. Both methods use experience to solve the reinforcement learning problem, but they differ in how and when they update their value estimates.\n"
      ],
      "metadata": {
        "id": "H5Fi7ieK66NR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Summary.jpg' width=800/>"
      ],
      "metadata": {
        "id": "oiExeyUd7a51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monte Carlo Learning\n",
        "\n",
        "Monte Carlo methods update the value function based on the entire episode of experience. This means that the agent interacts with the environment until the episode terminates, and then uses the total accumulated reward (return) to update the value function.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ar_P55By68aK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-5.jpg' width=800/>"
      ],
      "metadata": {
        "id": "LpsG7fT97b8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Characteristics:**\n",
        "- **Learning at the end of the episode:** The value function is updated only after the episode has concluded.\n",
        "- **Accurate but high variance:** Since Monte Carlo uses the actual return, the updates are accurate. However, the variance can be high due to the dependence on the entire episode.\n",
        "- **No bootstrapping:** Monte Carlo methods do not rely on previous estimates; they use the complete return for updates.\n",
        "\n"
      ],
      "metadata": {
        "id": "rfHK6zfm7DLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Process:**\n",
        "1. The agent follows a policy to generate an episode consisting of states, actions, rewards, and next states.\n",
        "2. At the end of the episode, the agent calculates the return $G_t$.\n",
        "3. The value function $V(S_t)$ is updated based on the return $G_t$.\n",
        "\n",
        "$$ V(S_t) = V(S_t) + \\alpha \\left[ G_t - V(S_t) \\right] $$"
      ],
      "metadata": {
        "id": "BZFysPq37E0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temporal Difference Learning\n",
        "\n",
        "Temporal Difference methods, such as TD(0), update the value function based on a single step of experience. The value function is updated incrementally using the difference between the estimated value and the actual reward plus the discounted value of the next state.\n",
        "\n"
      ],
      "metadata": {
        "id": "T7SohbnU7MYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-3.jpg' width=800/>"
      ],
      "metadata": {
        "id": "H0DrOqJD7cu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Characteristics:**\n",
        "- **Learning at each step:** The value function is updated after each interaction with the environment.\n",
        "- **Lower variance but biased:** TD methods can have lower variance because they bootstrap, using existing value estimates to update the value function. However, this introduces bias.\n",
        "- **Bootstrapping:** TD methods update the value function using the estimated return from the current state and action.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CBLZmgcs7RKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Process:**\n",
        "1. The agent takes an action $A_t$ in state $S_t$, receives a reward $R_{t+1}$, and transitions to the next state $S_{t+1}$.\n",
        "2. The value function $V(S_t)$ is updated based on the reward and the estimated value of $S_{t+1}$.\n",
        "\n",
        "$$ V(S_t) = V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right] $$"
      ],
      "metadata": {
        "id": "52uek33k7SXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img src=\"https://img.icons8.com/?size=50&id=110865&format=png&color=000000\" style=\"height:30px;display:inline\"> Q-Learning\n",
        "\n",
        "\n",
        "\n",
        "Q-Learning is a fundamental algorithm in reinforcement learning that helps in training an action-value function, which we commonly refer to as the Q-function. This Q-function determines the value of being at a particular state and taking a specific action at that state. The key characteristic of Q-Learning is that it uses a Temporal Difference (TD) approach to update its Q-values."
      ],
      "metadata": {
        "id": "KC4crPNg_2x7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Temporal Difference Learning as Stochastic Approximation\n",
        "\n",
        "Evaluating $Q^\\pi(s,a)$ involves estimating the expected return $G^\\pi(s,a)$. Stochastic approximation theory provides a way to estimate $Q^\\pi(s,a)$ from experience samples rather than from a model. This is achieved through a sequence of updates:\n",
        "\n",
        "$$ q_{t+1} = q_t + \\alpha_t \\left(g^\\pi_t - q_t\\right) $$\n",
        "\n",
        "where $g^\\pi_t$ are independent realizations of $G^\\pi(s,a)$ and $\\alpha_t$ are step sizes that meet specific conditions to ensure convergence.\n",
        "\n",
        "This approach can be generalized using Stochastic Gradient Descent (SGD), where $Q^\\pi$ minimizes the loss function:\n",
        "\n",
        "$$ L(Q) = \\frac{1}{2} \\int_{S \\times A} \\left[ Q(s,a) - \\mathbb{E}\\left(G^\\pi(s,a)\\right) \\right]^2 dsda $$\n",
        "\n",
        "The gradient descent update for this can be approximated using a set of independent state-action pairs, leading to the SGD update:\n",
        "\n",
        "$$ Q \\leftarrow Q - \\alpha \\sum_{i=1}^N \\left[ Q(s_i,a_i) - g^\\pi(s_i,a_i) \\right] \\nabla_Q Q(s_i,a_i) $$\n",
        "\n",
        "For simplicity, the stochastic approximation perspective is often used, where the update becomes:\n",
        "\n",
        "$$ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left(g^\\pi(s,a) - Q(s,a) \\right) $$\n"
      ],
      "metadata": {
        "id": "GzcLBjU7_5jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Temporal Difference (TD) Learning\n",
        "\n",
        "TD learning combines ideas from Monte Carlo methods and dynamic programming. It updates the value function after every step using the TD error, which measures the difference between the predicted value and the actual observed value. The TD(0) update for a Q-function is given by:\n",
        "\n",
        "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left(r_t + \\gamma Q(s_{t+1}, \\pi(s_{t+1})) - Q(s_t,a_t)\\right).$$\n",
        "\n",
        "Here, $r_t$ is the reward received, $\\gamma$ is the discount factor, and $\\pi$ is the policy being evaluated.\n",
        "\n",
        "**Temporal Difference Error**: The difference $\\delta_t = r_t + \\gamma Q(s_{t+1}, \\pi(s_{t+1})) - Q(s_t,a_t)$ is called the temporal difference (TD) error. It represents the discrepancy between the predicted Q-value and the observed return."
      ],
      "metadata": {
        "id": "jO0Cg46ZA3HC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bootstrapping and Convergence\n",
        "\n",
        "Bootstrapping refers to the use of existing value estimates to update Q-values. This is a core aspect of TD learning, enabling it to update values based on a combination of observed rewards and estimated future values.\n",
        "\n",
        "For TD(0) to converge to the correct Q-values, the following conditions must be met:\n",
        "- All state-action pairs must be visited infinitely often.\n",
        "- The step sizes $\\alpha_t$ must satisfy the Robbins-Monro conditions."
      ],
      "metadata": {
        "id": "b20M0QkCBitZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=tLpFC4eS6GLY&format=png&color=000000\" style=\"height:30px;display:inline\"> Q-Learning Algorithm\n",
        "\n"
      ],
      "metadata": {
        "id": "oBYVWN0mC41s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg' width=800/>"
      ],
      "metadata": {
        "id": "ApinyeqQEioV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/task_sign.png?raw=true' width=800/>"
      ],
      "metadata": {
        "id": "biggcRnT6aOs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pFhWblk3Awr"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=46589&format=png&color=000000\" style=\"height:30px;display:inline\"> Task 4: Create and Initialize the Q-table\n",
        "\n",
        "(Step 1 of the pseudocode)\n",
        "\n",
        "\n",
        "**Initialize the Q-Table**\n",
        "   - Create a table $Q$ with all state-action pairs, initialized to zero or some arbitrary value.\n",
        "   - This table will be updated as the agent learns from its interactions with the environment.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "y3ZCdluj3k0l"
      },
      "outputs": [],
      "source": [
        "%%script true\n",
        "\n",
        "state_space =\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "\n",
        "action_space =\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "rCddoOXM3UQH"
      },
      "outputs": [],
      "source": [
        "%%script true\n",
        "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros. np.zeros needs a tuple (a,b)\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable =\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "9YfvrqRt3jdR"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/solution_sign.png?raw=true' width=800/>"
      ],
      "metadata": {
        "id": "1UpXz5elHhEs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwDzzC26Dj1I"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/?size=50&id=42816&format=png&color=000000\" style=\"height:30px;display:inline\"> Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "HuTKv3th3ohG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae1716d-637c-4685-a904-22ca6367397a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are  16  possible states\n",
            "There are  4  possible actions\n"
          ]
        }
      ],
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "\n",
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "lnrb_nX33fJo"
      },
      "outputs": [],
      "source": [
        "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable = np.zeros((state_space, action_space))\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Y0WlgkVO3Jf9"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Atll4Z774gri"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=42957&format=png&color=000000\" style=\"height:30px;display:inline\"> Greedy policy\n",
        "\n",
        "\n",
        "\n",
        "Remember we have two policies since Q-Learning is an **off-policy** algorithm. This means we're using a **different policy for acting and updating the value function**.\n",
        "\n",
        "- Epsilon-greedy policy (acting policy)\n",
        "- Greedy-policy (updating policy)\n",
        "\n",
        "The greedy policy will also be the final policy we'll have when the Q-learning agent completes training. The greedy policy is used to select an action using the Q-table.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <img src=\"https://img.icons8.com/?size=50&id=46589&format=png&color=000000\" style=\"height:30px;display:inline\"> Task 5: Define the greedy policy"
      ],
      "metadata": {
        "id": "aQjXFDlPBdX5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "E3SCLmLX5bWG"
      },
      "outputs": [],
      "source": [
        "%%script true\n",
        "\n",
        "def greedy_policy(Qtable, state):\n",
        "  # Exploitation: take the action with the highest state, action value\n",
        "  action =\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/solution_sign.png?raw=true' width=800/>"
      ],
      "metadata": {
        "id": "4z1wKOSHHh9G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGnSDymXDmFA"
      },
      "source": [
        "#### <img src=\"https://img.icons8.com/?size=50&id=42816&format=png&color=000000\" style=\"height:30px;display:inline\"> Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "se2OzWGW5kYJ"
      },
      "outputs": [],
      "source": [
        "def greedy_policy(Qtable, state):\n",
        "  # Exploitation: take the action with the highest state, action value\n",
        "  action = np.argmax(Qtable[state][:])\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flILKhBU3yZ7"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=fMIqNhC5krWZ&format=png&color=000000\" style=\"height:30px;display:inline\"> Epsilon-greedy policy\n",
        "\n",
        "\n",
        "\n",
        "Epsilon-greedy is the training policy that handles the exploration/exploitation trade-off.\n",
        "\n",
        "The idea with epsilon-greedy:\n",
        "\n",
        "- With *probability 1‚Ää-‚Ää…õ* : **we do exploitation** (i.e. our agent selects the action with the highest state-action pair value).\n",
        "\n",
        "- With *probability …õ*: we do **exploration** (trying a random action).\n",
        "\n",
        "As the training continues, we progressively **reduce the epsilon value since we will need less and less exploration and more exploitation.**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exploration vs. Exploitation Tradeoff\n",
        "\n",
        "Balancing exploration and exploitation is crucial in Q-Learning. The $\\epsilon$-greedy strategy is commonly used to address this tradeoff:\n",
        "- **Exploration**: Trying new actions to discover their effects and gather more information.\n",
        "- **Exploitation**: Using known information to maximize the immediate reward.\n",
        "\n",
        "\n",
        "At the beginning of the training, the probability of doing exploration will be huge since …õ is very high, so most of the time, we‚Äôll explore. But as the training goes on, and consequently our Q-table gets better and better in its estimations, we progressively reduce the epsilon value since we will need less and less exploration and more exploitation.\n",
        "\n",
        "As training progresses, the value of $\\epsilon$ is gradually reduced to shift the focus from exploration to exploitation, leveraging the learned Q-values to make better decisions."
      ],
      "metadata": {
        "id": "LpqFsTobG__m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-5.jpg\" alt=\"Q-Learning\" width=\"70%\"/>"
      ],
      "metadata": {
        "id": "C1Q_l0rPHdlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <img src=\"https://img.icons8.com/?size=50&id=46589&format=png&color=000000\" style=\"height:30px;display:inline\"> Task 6: Define the epsilon greedy policy"
      ],
      "metadata": {
        "id": "nnURA7RvCbFC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "6Bj7x3in3_Pq"
      },
      "outputs": [],
      "source": [
        "%%script true\n",
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  # Randomly generate a number between 0 and 1\n",
        "  random_num =\n",
        "  # if random_num > greater than epsilon --> exploitation\n",
        "  if random_num > epsilon:\n",
        "    # Take the action with the highest value given a state\n",
        "    # np.argmax can be useful here\n",
        "    action =\n",
        "  # else --> exploration\n",
        "  else:\n",
        "    action = # Take a random action\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/solution_sign.png?raw=true' width=800/>"
      ],
      "metadata": {
        "id": "X4rONBMyHjEy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US4ofTs8Do8x"
      },
      "source": [
        "#### <img src=\"https://img.icons8.com/?size=50&id=42816&format=png&color=000000\" style=\"height:30px;display:inline\"> Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "cYxHuckr4LiG"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  # Randomly generate a number between 0 and 1\n",
        "  random_num = random.uniform(0,1)\n",
        "  # if random_num > greater than epsilon --> exploitation\n",
        "  if random_num > epsilon:\n",
        "    # Take the action with the highest value given a state\n",
        "    # np.argmax can be useful here\n",
        "    action = greedy_policy(Qtable, state)\n",
        "  # else --> exploration\n",
        "  else:\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDb7Tdx8atfL"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=104321&format=png&color=000000\" style=\"height:30px;display:inline\"> The training loop method\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://cdn-media-1.freecodecamp.org/images/TnN7ys7VGKoDszzv3WDnr5H8txOj3KKQ0G8o\">\n",
        "\n",
        "The training loop goes like this:\n",
        "\n",
        "```\n",
        "For episode in the total of training episodes:\n",
        "\n",
        "Reduce epsilon (since we need less and less exploration)\n",
        "Reset the environment\n",
        "\n",
        "  For step in max timesteps:    \n",
        "    Choose the action At using epsilon greedy policy\n",
        "    Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "    Update the Q-value Q(s,a) using Bellman equation Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "    If done, finish the episode\n",
        "    Our next state is the new state\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <img src=\"https://img.icons8.com/?size=50&id=46589&format=png&color=000000\" style=\"height:30px;display:inline\"> Task 7: Create the training loop"
      ],
      "metadata": {
        "id": "x9hsIjWuDVWd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "paOynXy3aoJW"
      },
      "outputs": [],
      "source": [
        "%%script true\n",
        "\n",
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in tqdm(range(n_training_episodes)):\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # Reset the environment\n",
        "    state, info = env.reset()\n",
        "    step = 0\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    # repeat\n",
        "    for step in range(max_steps):\n",
        "      # Choose the action At using epsilon greedy policy\n",
        "      action =\n",
        "\n",
        "      # Take action At and observe Rt+1 and St+1\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "      new_state, reward, terminated, truncated, info =\n",
        "\n",
        "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "      Qtable[state][action] =\n",
        "\n",
        "      # If terminated or truncated finish the episode\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "      # Our next state is the new state\n",
        "      state = new_state\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/CLAIR-LAB-TECHNION/CLAI/blob/main/tutorials/assets/solution_sign.png?raw=true' width=800/>"
      ],
      "metadata": {
        "id": "IY8FFQNPHkay"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCMNGbUfDtwY"
      },
      "source": [
        "#### <img src=\"https://img.icons8.com/?size=50&id=42816&format=png&color=000000\" style=\"height:30px;display:inline\"> Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "IyZaYbUAeolw"
      },
      "outputs": [],
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in tqdm(range(n_training_episodes)):\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # Reset the environment\n",
        "    state, info = env.reset()\n",
        "    step = 0\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    # repeat\n",
        "    for step in range(max_steps):\n",
        "      # Choose the action At using epsilon greedy policy\n",
        "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
        "\n",
        "      # Take action At and observe Rt+1 and St+1\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
        "\n",
        "      # If terminated or truncated finish the episode\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "      # Our next state is the new state\n",
        "      state = new_state\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW80DealcRtu"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=43434&format=png&color=000000\" style=\"height:30px;display:inline\"> Define the hyperparameters\n",
        "\n",
        "\n",
        "\n",
        "The exploration related hyperparamters are some of the most important ones.\n",
        "\n",
        "- We need to make sure that our agent **explores enough of the state space** to learn a good value approximation. To do that, we need to have progressive decay of the epsilon.\n",
        "- If you decrease epsilon too fast (too high decay_rate), **you take the risk that your agent will be stuck**, since your agent didn't explore enough of the state space and hence can't solve the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Y1tWn0tycWZ1"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "n_training_episodes = 10000  # Total training episodes\n",
        "learning_rate = 0.7          # Learning rate\n",
        "\n",
        "# Evaluation parameters\n",
        "n_eval_episodes = 100        # Total number of test episodes\n",
        "\n",
        "# Environment parameters\n",
        "env_id = \"FrozenLake-v1\"     # Name of the environment\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.95                 # Discounting rate\n",
        "eval_seed = []               # The evaluation seed of the environment\n",
        "\n",
        "# Exploration parameters\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.05            # Minimum exploration probability\n",
        "decay_rate = 0.0005            # Exponential decay rate for exploration prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLwKQ4tUdhGI"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=xqc9b4R1mo2b&format=png&color=000000\" style=\"height:30px;display:inline\"> Train the Q-Learning agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "DPBxfjJdTCOH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b1f65db482314fc4ac9ac211b5d2da33",
            "ab8a85460f1b41c1a2d8c9a783273b62",
            "ff95a49d8d1948eab9950597cb29d33c",
            "698ccdf8f30c47a9b24ba5b609c29a1e",
            "0ba824599ff3460e805f94f6cbd5a39b",
            "794fba50af52453a9c9e62ec9e01c43e",
            "9da491d88152482791b0e2faa2130299",
            "4f0ed3bf753a40dcb8938123fc68ae0a",
            "a6e47fb4f8e64da9875368114cc15aac",
            "1f046f60c3864f90a2799619c42c97a3",
            "0abdf800dfd0491f98bcdc8bf9acc017"
          ]
        },
        "outputId": "88843cdd-6a0d-470a-8e95-f893347de12c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1f65db482314fc4ac9ac211b5d2da33"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVeEhUCrc30L"
      },
      "source": [
        "Let's see what our Q-Learning table looks like now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "nmfchsTITw4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f554785-4a7b-4372-b1ba-92aa11b259d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n",
              "       [0.73509189, 0.        , 0.81450625, 0.77378094],\n",
              "       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n",
              "       [0.81450625, 0.        , 0.77378094, 0.77378094],\n",
              "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.9025    , 0.        , 0.81450625],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
              "       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
              "       [0.857375  , 0.95      , 0.        , 0.857375  ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
              "       [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
              "       [0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "Qtable_frozenlake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUrWkxsHccXD"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/?size=50&id=43157&format=png&color=000000\" style=\"height:30px;display:inline\">  The evaluation method\n",
        "\n",
        "\n",
        "\n",
        "- We defined the evaluation method that we're going to use to test our Q-Learning agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "jNl0_JO2cbkm"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
        "  \"\"\"\n",
        "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "  :param env: The evaluation environment\n",
        "  :param max_steps: Maximum number of steps per episode\n",
        "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "  :param Q: The Q-table\n",
        "  :param seed: The evaluation seed array (for taxi-v3)\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in tqdm(range(n_eval_episodes)):\n",
        "    if seed:\n",
        "      state, info = env.reset(seed=seed[episode])\n",
        "    else:\n",
        "      state, info = env.reset()\n",
        "    step = 0\n",
        "    truncated = False\n",
        "    terminated = False\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      # Take the action (index) that have the maximum expected future reward given that state\n",
        "      action = greedy_policy(Q, state)\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convergence\n",
        "\n",
        "Q-Learning converges to the optimal Q-values $Q^*$ under certain conditions:\n",
        "- **Exploration**: All state-action pairs must be visited infinitely often to ensure comprehensive learning.\n",
        "- **Learning Rate**: The learning rate $\\alpha$ must meet the Robbins-Monro conditions: $\\sum_t \\alpha_t = \\infty$ and $\\sum_t \\alpha_t^2 < \\infty$. These conditions ensure that the learning rate decreases over time, allowing for stable updates.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z-dYBxQMG4R8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jJqjaoAnxUo"
      },
      "source": [
        "#### Evaluate our Q-Learning agent\n",
        "\n",
        "- Usually, you should have a mean reward of 1.0\n",
        "- The **environment is relatively easy** since the state space is really small (16). What you can try to do is [to replace it with the slippery version](https://gymnasium.farama.org/environments/toy_text/frozen_lake/), which introduces stochasticity, making the environment more complex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "fAgB7s0HEFMm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "349c089a7ef14117927eb3ffbc8e4a8c",
            "5e77cb480c064053859d9761ef1e5d41",
            "25298d2cd9cc415c98543e71726c2a85",
            "e97f090e5d654b4aaa45097b3c33919a",
            "775fd2e4d7484c9d9e3e29b819d98abb",
            "d2505ae2d01642a099b295ccb5023392",
            "d6ffaec2fe864fd2bee3c171554f0337",
            "4384159105a94c158206e60768bcf0ee",
            "c282fdcefed54703bda18e9041dbfe01",
            "332c084c63614c9e9a231cb1cf1a3728",
            "6b2e4f2df4da4dd3b9dd09474b14c214"
          ]
        },
        "outputId": "f6cb00cb-6069-4411-f15b-eec93cf856ce"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "349c089a7ef14117927eb3ffbc8e4a8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean_reward=1.00 +/- 0.00\n"
          ]
        }
      ],
      "source": [
        "# Evaluate our Agent\n",
        "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
        "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f175852a-f29b-430a-97b8-6565bc3a503f"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/?size=100&id=46509&format=png&color=000000\" style=\"height:50px;display:inline\"> Conclusion\n",
        "---\n",
        "\n",
        "In this tutorial, we delved into several key concepts and methodologies fundamental to value-based reinforcement learning. Our journey began with an exploration of value functions, the backbone of understanding the expected return from states and actions under a given policy. We then introduced the Bellman equations, which provide a recursive decomposition of value functions and are pivotal for various RL algorithms.\n",
        "\n",
        "We moved on to Temporal Difference (TD) learning, a powerful approach that combines the ideas of Monte Carlo methods and dynamic programming to update value estimates based on observed rewards. This method serves as the foundation for many RL algorithms, including the widely used Q-learning.\n",
        "\n",
        "Value iteration was also discussed as a method for computing optimal policies by iteratively improving value function estimates. We examined how this process converges to the optimal value function, providing a robust way to solve Markov Decision Processes (MDPs).\n",
        "\n",
        "Finally, we focused on Q-learning, a model-free, off-policy algorithm that updates Q-values using the Bellman optimality equation. This method allows agents to learn optimal policies through interaction with the environment, balancing exploration and exploitation to improve decision-making over time.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_Ndmee2iL02"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/dusk/64/000000/plus-2-math.png\" style=\"height:50px;display:inline\"> Further Reading\n",
        "---\n",
        "\n",
        "\n",
        "For a deeper understanding of value-based methods and Q-Learning, explore the following resources:\n",
        "\n",
        "- **Hugging Face Deep RL Course - Q-Learning**: A comprehensive guide to Q-Learning, covering theory and practical implementation. [Read more](https://huggingface.co/learn/deep-rl-course/unit2/q-learning)\n",
        "- **Towards Data Science - Q-Learning Algorithm**: An intuitive explanation and step-by-step implementation of Q-Learning in Python. [Read more](https://towardsdatascience.com/q-learning-algorithm-from-explanation-to-implementation-cdbeda2ea187)\n",
        "- **Advanced Readings in Reinforcement Learning**: Detailed proofs and theoretical discussions on advanced RL topics. [Access the document](https://drive.google.com/file/d/1004d-GVvXuI7vgLpzPvoESeP5jyERw9Z/view)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxLYbw1wj9wS"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/?size=100&id=46756&format=png&color=000000\" style=\"height:50px;display:inline\"> Credits\n",
        "---\n",
        "* Examples and code snippets were taken from <a href=\"https://huggingface.co/learn/deep-rl-course/unit4/introduction\"> Hugging Face Deep RL Course </a>\n",
        "* Examples and explanations were taken from <a href=\"https://rail.eecs.berkeley.edu/deeprlcourse/\">CS285 - Deep Reinforcement Learning Course at UC Berkeley</a>\n",
        "* Icons from <a href=\"https://icons8.com/\">Icons8.com"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "12px",
        "width": "186px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "410.667px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "-oid7uNNGKvv",
        "UoXl-zQ5bxxG",
        "r4P79ZolWypl",
        "pbIQ15HYzdLu",
        "eypSYcGfzdLu",
        "S34VKH9x2Fy4",
        "MVqn4Kc8Aomo",
        "sJBNkA_h64g1"
      ]
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b1f65db482314fc4ac9ac211b5d2da33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab8a85460f1b41c1a2d8c9a783273b62",
              "IPY_MODEL_ff95a49d8d1948eab9950597cb29d33c",
              "IPY_MODEL_698ccdf8f30c47a9b24ba5b609c29a1e"
            ],
            "layout": "IPY_MODEL_0ba824599ff3460e805f94f6cbd5a39b"
          }
        },
        "ab8a85460f1b41c1a2d8c9a783273b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_794fba50af52453a9c9e62ec9e01c43e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9da491d88152482791b0e2faa2130299",
            "value": "100%"
          }
        },
        "ff95a49d8d1948eab9950597cb29d33c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f0ed3bf753a40dcb8938123fc68ae0a",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6e47fb4f8e64da9875368114cc15aac",
            "value": 10000
          }
        },
        "698ccdf8f30c47a9b24ba5b609c29a1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f046f60c3864f90a2799619c42c97a3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0abdf800dfd0491f98bcdc8bf9acc017",
            "value": "‚Äá10000/10000‚Äá[00:02&lt;00:00,‚Äá4427.08it/s]"
          }
        },
        "0ba824599ff3460e805f94f6cbd5a39b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "794fba50af52453a9c9e62ec9e01c43e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9da491d88152482791b0e2faa2130299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f0ed3bf753a40dcb8938123fc68ae0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6e47fb4f8e64da9875368114cc15aac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f046f60c3864f90a2799619c42c97a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0abdf800dfd0491f98bcdc8bf9acc017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "349c089a7ef14117927eb3ffbc8e4a8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e77cb480c064053859d9761ef1e5d41",
              "IPY_MODEL_25298d2cd9cc415c98543e71726c2a85",
              "IPY_MODEL_e97f090e5d654b4aaa45097b3c33919a"
            ],
            "layout": "IPY_MODEL_775fd2e4d7484c9d9e3e29b819d98abb"
          }
        },
        "5e77cb480c064053859d9761ef1e5d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2505ae2d01642a099b295ccb5023392",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d6ffaec2fe864fd2bee3c171554f0337",
            "value": "100%"
          }
        },
        "25298d2cd9cc415c98543e71726c2a85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4384159105a94c158206e60768bcf0ee",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c282fdcefed54703bda18e9041dbfe01",
            "value": 100
          }
        },
        "e97f090e5d654b4aaa45097b3c33919a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_332c084c63614c9e9a231cb1cf1a3728",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6b2e4f2df4da4dd3b9dd09474b14c214",
            "value": "‚Äá100/100‚Äá[00:00&lt;00:00,‚Äá2576.75it/s]"
          }
        },
        "775fd2e4d7484c9d9e3e29b819d98abb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2505ae2d01642a099b295ccb5023392": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6ffaec2fe864fd2bee3c171554f0337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4384159105a94c158206e60768bcf0ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c282fdcefed54703bda18e9041dbfe01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "332c084c63614c9e9a231cb1cf1a3728": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b2e4f2df4da4dd3b9dd09474b14c214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}